# 21ì¥ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬


[<img src="https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png" align="left"/> ](https://colab.research.google.com/github/taehojo/deeplearning_4th/blob/master/colab/ch21-colab.ipynb)


## 1. í…ìŠ¤íŠ¸ì˜ í† í°í™”


```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,Embedding
from tensorflow.keras.utils import to_categorical
from numpy import array

# ì¼€ë¼ìŠ¤ì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ì™€ ê´€ë ¨í•œ í•¨ìˆ˜ì¤‘ text_to_word_sequence í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
from tensorflow.keras.preprocessing.text import text_to_word_sequence
 
# ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ë¥¼ ì •í•©ë‹ˆë‹¤.
text = 'í•´ë³´ì§€ ì•Šìœ¼ë©´ í•´ë‚¼ ìˆ˜ ì—†ë‹¤'
 
# í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.
result = text_to_word_sequence(text)
print("\nì›ë¬¸:\n", text)
print("\ní† í°í™”:\n", result)
```


**ì¶œë ¥ ê²°ê³¼:**


```

ì›ë¬¸:
 í•´ë³´ì§€ ì•Šìœ¼ë©´ í•´ë‚¼ ìˆ˜ ì—†ë‹¤

í† í°í™”:
 ['í•´ë³´ì§€', 'ì•Šìœ¼ë©´', 'í•´ë‚¼', 'ìˆ˜', 'ì—†ë‹¤']

```


```python
# ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì„¸ê¸°

# ì „ì²˜ë¦¬í•˜ë ¤ëŠ” ì„¸ ê°œì˜ ë¬¸ì¥ì„ ì •í•©ë‹ˆë‹¤.
docs = ['ë¨¼ì € í…ìŠ¤íŠ¸ì˜ ê° ë‹¨ì–´ë¥¼ ë‚˜ëˆ„ì–´ í† í°í™” í•©ë‹ˆë‹¤.',
       'í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ë¡œ í† í°í™”í•´ì•¼ ë”¥ëŸ¬ë‹ì—ì„œ ì¸ì‹ë©ë‹ˆë‹¤.',
       'í† í°í™”í•œ ê²°ê³¼ëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
       ]
 
# í† í°í™” í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì „ì²˜ë¦¬ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
token = Tokenizer()             # í† í°í™” í•¨ìˆ˜ ì§€ì •
token.fit_on_texts(docs)       # í† í°í™” í•¨ìˆ˜ì— ë¬¸ì¥ ì ìš©
 
# ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ë¥¼ ê° ì˜µì…˜ì— ë§ì¶”ì–´ ì¶œë ¥í•©ë‹ˆë‹¤.
# Tokenizer()ì˜ word_counts í•¨ìˆ˜ëŠ” ìˆœì„œë¥¼ ê¸°ì–µí•˜ëŠ” OrderedDict í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
print("\në‹¨ì–´ ì¹´ìš´íŠ¸:\n", token.word_counts) 

# ì¶œë ¥ë˜ëŠ” ìˆœì„œëŠ” ëœë¤ì…ë‹ˆë‹¤. 
print("\në¬¸ì¥ ì¹´ìš´íŠ¸: ", token.document_count)
print("\nê° ë‹¨ì–´ê°€ ëª‡ ê°œì˜ ë¬¸ì¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ê°€:\n", token.word_docs)
print("\nê° ë‹¨ì–´ì— ë§¤ê²¨ì§„ ì¸ë±ìŠ¤ ê°’:\n",  token.word_index)
```


**ì¶œë ¥ ê²°ê³¼:**


```

ë‹¨ì–´ ì¹´ìš´íŠ¸:
 OrderedDict([('ë¨¼ì €', 1), ('í…ìŠ¤íŠ¸ì˜', 2), ('ê°', 1), ('ë‹¨ì–´ë¥¼', 1), ('ë‚˜ëˆ„ì–´', 1), ('í† í°í™”', 1), ('í•©ë‹ˆë‹¤', 1), ('ë‹¨ì–´ë¡œ', 1), ('í† í°í™”í•´ì•¼', 1), ('ë”¥ëŸ¬ë‹ì—ì„œ', 2), ('ì¸ì‹ë©ë‹ˆë‹¤', 1), ('í† í°í™”í•œ', 1), ('ê²°ê³¼ëŠ”', 1), ('ì‚¬ìš©í• ', 1), ('ìˆ˜', 1), ('ìˆìŠµë‹ˆë‹¤', 1)])

ë¬¸ì¥ ì¹´ìš´íŠ¸:  3

ê° ë‹¨ì–´ê°€ ëª‡ ê°œì˜ ë¬¸ì¥ì— í¬í•¨ë˜ì–´ ìˆëŠ”ê°€:
 defaultdict(<class 'int'>, {'í…ìŠ¤íŠ¸ì˜': 2, 'ë‚˜ëˆ„ì–´': 1, 'ë¨¼ì €': 1, 'í† í°í™”': 1, 'í•©ë‹ˆë‹¤': 1, 'ê°': 1, 'ë‹¨ì–´ë¥¼': 1, 'ë‹¨ì–´ë¡œ': 1, 'ë”¥ëŸ¬ë‹ì—ì„œ': 2, 'ì¸ì‹ë©ë‹ˆë‹¤': 1, 'í† í°í™”í•´ì•¼': 1, 'ì‚¬ìš©í• ': 1, 'ìˆ˜': 1, 'ê²°ê³¼ëŠ”': 1, 'í† í°í™”í•œ': 1, 'ìˆìŠµë‹ˆë‹¤': 1})

ê° ë‹¨ì–´ì— ë§¤ê²¨ì§„ ì¸ë±ìŠ¤ ê°’:
 {'í…ìŠ¤íŠ¸ì˜': 1, 'ë”¥ëŸ¬ë‹ì—ì„œ': 2, 'ë¨¼ì €': 3, 'ê°': 4, 'ë‹¨ì–´ë¥¼': 5, 'ë‚˜ëˆ„ì–´': 6, 'í† í°í™”': 7, 'í•©ë‹ˆë‹¤': 8, 'ë‹¨ì–´ë¡œ': 9, 'í† í°í™”í•´ì•¼': 10, 'ì¸ì‹ë©ë‹ˆë‹¤': 11, 'í† í°í™”í•œ': 12, 'ê²°ê³¼ëŠ”': 13, 'ì‚¬ìš©í• ': 14, 'ìˆ˜': 15, 'ìˆìŠµë‹ˆë‹¤': 16}

```


## 2. ë‹¨ì–´ì˜ ì›-í•« ì¸ì½”ë”©


```python
text="ì˜¤ë«ë™ì•ˆ ê¿ˆê¾¸ëŠ” ì´ëŠ” ê·¸ ê¿ˆì„ ë‹®ì•„ê°„ë‹¤"
token = Tokenizer()
token.fit_on_texts([text])
print(token.word_index)
```


**ì¶œë ¥ ê²°ê³¼:**


```
{'ì˜¤ë«ë™ì•ˆ': 1, 'ê¿ˆê¾¸ëŠ”': 2, 'ì´ëŠ”': 3, 'ê·¸': 4, 'ê¿ˆì„': 5, 'ë‹®ì•„ê°„ë‹¤': 6}

```


```python
x=token.texts_to_sequences([text])
print(x)
```


**ì¶œë ¥ ê²°ê³¼:**


```
[[1, 2, 3, 4, 5, 6]]

```


```python
#ì¸ë±ìŠ¤ ìˆ˜ì— í•˜ë‚˜ë¥¼ ì¶”ê°€í•´ì„œ ì›-í•« ì¸ì½”ë”© ë°°ì—´ ë§Œë“¤ê¸°
word_size = len(token.word_index) + 1
x = to_categorical(x, num_classes=word_size)
print(x)
```


**ì¶œë ¥ ê²°ê³¼:**


```
[[[0. 1. 0. 0. 0. 0. 0.]
  [0. 0. 1. 0. 0. 0. 0.]
  [0. 0. 0. 1. 0. 0. 0.]
  [0. 0. 0. 0. 1. 0. 0.]
  [0. 0. 0. 0. 0. 1. 0.]
  [0. 0. 0. 0. 0. 0. 1.]]]

```


## 4.í…ìŠ¤íŠ¸ë¥¼ ì½ê³  ê¸ì •, ë¶€ì • ì˜ˆì¸¡í•˜ê¸°


```python
# í…ìŠ¤íŠ¸ ë¦¬ë·° ìë£Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
docs = ["ë„ˆë¬´ ì¬ë°Œë„¤ìš”","ìµœê³ ì˜ˆìš”","ì°¸ ì˜ ë§Œë“  ì˜í™”ì˜ˆìš”","ì¶”ì²œí•˜ê³  ì‹¶ì€ ì˜í™”ì…ë‹ˆë‹¤","í•œë²ˆ ë” ë³´ê³ ì‹¶ë„¤ìš”","ê¸€ì„ìš”","ë³„ë¡œì˜ˆìš”","ìƒê°ë³´ë‹¤ ì§€ë£¨í•˜ë„¤ìš”","ì—°ê¸°ê°€ ì–´ìƒ‰í•´ìš”","ì¬ë¯¸ì—†ì–´ìš”"]

# ê¸ì • ë¦¬ë·°ëŠ” 1, ë¶€ì • ë¦¬ë·°ëŠ” 0ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
classes = array([1,1,1,1,1,0,0,0,0,0])

# í† í°í™” 
token = Tokenizer()
token.fit_on_texts(docs)
print(token.word_index)

```


**ì¶œë ¥ ê²°ê³¼:**


```
{'ë„ˆë¬´': 1, 'ì¬ë°Œë„¤ìš”': 2, 'ìµœê³ ì˜ˆìš”': 3, 'ì°¸': 4, 'ì˜': 5, 'ë§Œë“ ': 6, 'ì˜í™”ì˜ˆìš”': 7, 'ì¶”ì²œí•˜ê³ ': 8, 'ì‹¶ì€': 9, 'ì˜í™”ì…ë‹ˆë‹¤': 10, 'í•œë²ˆ': 11, 'ë”': 12, 'ë³´ê³ ì‹¶ë„¤ìš”': 13, 'ê¸€ì„ìš”': 14, 'ë³„ë¡œì˜ˆìš”': 15, 'ìƒê°ë³´ë‹¤': 16, 'ì§€ë£¨í•˜ë„¤ìš”': 17, 'ì—°ê¸°ê°€': 18, 'ì–´ìƒ‰í•´ìš”': 19, 'ì¬ë¯¸ì—†ì–´ìš”': 20}

```


```python
x = token.texts_to_sequences(docs)
print("\në¦¬ë·° í…ìŠ¤íŠ¸, í† í°í™” ê²°ê³¼:\n",  x)
```


**ì¶œë ¥ ê²°ê³¼:**


```

ë¦¬ë·° í…ìŠ¤íŠ¸, í† í°í™” ê²°ê³¼:
 [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]

```


```python
# íŒ¨ë”©, ì„œë¡œ ë‹¤ë¥¸ ê¸¸ì´ì˜ ë°ì´í„°ë¥¼ 4ë¡œ ë§ì¶”ì–´ ì¤ë‹ˆë‹¤.
padded_x = pad_sequences(x, 4)  
print("\níŒ¨ë”© ê²°ê³¼:\n", padded_x)
```


**ì¶œë ¥ ê²°ê³¼:**


```

íŒ¨ë”© ê²°ê³¼:
 [[ 0  0  1  2]
 [ 0  0  0  3]
 [ 4  5  6  7]
 [ 0  8  9 10]
 [ 0 11 12 13]
 [ 0  0  0 14]
 [ 0  0  0 15]
 [ 0  0 16 17]
 [ 0  0 18 19]
 [ 0  0  0 20]]

```


```python
# ì„ë² ë”©ì— ì…ë ¥ë  ë‹¨ì–´ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
word_size = len(token.word_index) +1

# ë‹¨ì–´ ì„ë² ë”©ì„ í¬í•¨í•˜ì—¬ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
model = Sequential()
model.add(Embedding(word_size, 8))
model.build((None, 4))
model.add(Flatten())
model.add(Dense(1, activation='sigmoid'))
model.summary()
```


**ì¶œë ¥ ê²°ê³¼:**


<div class="output_html">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "sequential"</span>
</pre>

</div>
<div class="output_html">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ<span style="font-weight: bold"> Layer (type)                    </span>â”ƒ<span style="font-weight: bold"> Output Shape           </span>â”ƒ<span style="font-weight: bold">       Param # </span>â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ embedding (<span style="color: #0087ff; text-decoration-color: #0087ff">Embedding</span>)           â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">4</span>, <span style="color: #00af00; text-decoration-color: #00af00">8</span>)           â”‚           <span style="color: #00af00; text-decoration-color: #00af00">168</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)             â”‚             <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)              â”‚            <span style="color: #00af00; text-decoration-color: #00af00">33</span> â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>

</div>
<div class="output_html">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">201</span> (804.00 B)
</pre>

</div>
<div class="output_html">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">201</span> (804.00 B)
</pre>

</div>
<div class="output_html">
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>

</div>


```python
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(padded_x, classes, epochs=20)
print("\n Accuracy: %.4f" % (model.evaluate(padded_x, classes)[1]))
```


**ì¶œë ¥ ê²°ê³¼:**


```
Epoch 1/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 560ms/step - accuracy: 0.6000 - loss: 0.6906
Epoch 2/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 38ms/step - accuracy: 0.7000 - loss: 0.6883
Epoch 3/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 40ms/step - accuracy: 0.8000 - loss: 0.6860
Epoch 4/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 41ms/step - accuracy: 0.8000 - loss: 0.6838
Epoch 5/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 42ms/step - accuracy: 0.8000 - loss: 0.6815
Epoch 6/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step - accuracy: 0.8000 - loss: 0.6792
Epoch 7/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 52ms/step - accuracy: 0.9000 - loss: 0.6770
Epoch 8/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step - accuracy: 0.9000 - loss: 0.6747
Epoch 9/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step - accuracy: 1.0000 - loss: 0.6724
Epoch 10/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 50ms/step - accuracy: 1.0000 - loss: 0.6701
Epoch 11/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step - accuracy: 1.0000 - loss: 0.6679
Epoch 12/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 63ms/step - accuracy: 1.0000 - loss: 0.6656
Epoch 13/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 55ms/step - accuracy: 1.0000 - loss: 0.6633
Epoch 14/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 53ms/step - accuracy: 1.0000 - loss: 0.6610
Epoch 15/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 47ms/step - accuracy: 1.0000 - loss: 0.6587
Epoch 16/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 55ms/step - accuracy: 1.0000 - loss: 0.6564
Epoch 17/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 60ms/step - accuracy: 1.0000 - loss: 0.6541
Epoch 18/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 49ms/step - accuracy: 1.0000 - loss: 0.6518
Epoch 19/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 44ms/step - accuracy: 1.0000 - loss: 0.6494
Epoch 20/20
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 45ms/step - accuracy: 0.9000 - loss: 0.6471
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 140ms/step - accuracy: 0.9000 - loss: 0.6447

 Accuracy: 0.9000

```

