# 24ì¥ íŠ¸ëœìŠ¤í¬ë¨¸ ì§ì ‘ ë§Œë“¤ì–´ ë³´ê¸°


## íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë”ë¥¼ í™œìš©í•œ ê¸ì • ë¶€ì • ì˜ˆì¸¡



[<img src="https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png" align="left"/> ](https://colab.research.google.com/github/taehojo/deeplearning_4th/blob/master/colab/ch24-colab.ipynb)


```python
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input
from tensorflow.keras.optimizers import Adam
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# ê¹ƒí—ˆë¸Œì— ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
!git clone https://github.com/taehojo/data.git

# CSV íŒŒì¼ ë¡œë“œ
dataframe = pd.read_csv('./data/sentiment_data.csv')

# ë°ì´í„°ì™€ ë¼ë²¨ ì¶”ì¶œ
sentences = dataframe['sentence'].tolist()
labels = dataframe['label'].tolist()

# ì„ë² ë”© ë²¡í„° í¬ê¸°ì™€ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ì„¤ì •
embedding_dim = 128
max_len = 10

# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index

# íŒ¨ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤
data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')

# ë°ì´í„°ì…‹ì„ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# í¬ì§€ì…”ë„ ì¸ì½”ë”© í•¨ìˆ˜
def get_positional_encoding(max_len, d_model):
    pos_enc = np.zeros((max_len, d_model))
    for pos in range(max_len):
        for i in range(0, d_model, 2):
            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))
            if i + 1 < d_model:
                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))
    return pos_enc

# í¬ì§€ì…”ë„ ì¸ì½”ë”© ìƒì„±
positional_encoding = get_positional_encoding(max_len, embedding_dim)

# ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´ (ë§ˆìŠ¤í¬ ë¯¸ì‚¬ìš©)
class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, num_heads, key_dim):
        super(MultiHeadSelfAttentionLayer, self).__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
        self.norm = LayerNormalization()

    def call(self, x):
        attn_output = self.mha(query=x, value=x, key=x)
        attn_output = self.norm(attn_output + x)
        return attn_output

# ëª¨ë¸ ì„¤ì •
inputs = Input(shape=(max_len,))

# 1. ì„ë² ë”© ë ˆì´ì–´
embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim)
embedded_sequences = embedding_layer(inputs)

# 2. í¬ì§€ì…”ë„ ì¸ì½”ë”© ì¶”ê°€
embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding

# 3. ì²« ë²ˆì§¸ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (ë§ˆìŠ¤í¬ ì—†ìŒ)
attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)
attention_output = attention_layer(embedded_sequences_with_positional_encoding)

# 4. ì”ì°¨ ì—°ê²°
attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])

# 5. GlobalAveragePooling1D
pooled_output = GlobalAveragePooling1D()(attention_output_with_residual)

# 6. í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
dense_layer = Dense(128, activation='relu')(pooled_output)
dropout_layer = Dropout(0.5)(dense_layer)
output_layer = Dense(1, activation='sigmoid')(dropout_layer)

model = Model(inputs=inputs, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))

# ìƒˆ ë¬¸ì¥ ì˜ˆì¸¡
sample_texts = ["I absolutely love this!", "I can't stand this product"]
sample_sequences = tokenizer.texts_to_sequences(sample_texts)
sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')
predictions = model.predict(sample_data)

for i, text in enumerate(sample_texts):
    print(f"Text: {text}")
    print(f"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}")

```


**ì¶œë ¥ ê²°ê³¼:**


```
Epoch 1/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 19ms/step - accuracy: 0.5680 - loss: 0.8149 - val_accuracy: 0.9975 - val_loss: 0.0172
Epoch 2/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 17ms/step - accuracy: 0.9898 - loss: 0.0427 - val_accuracy: 0.9975 - val_loss: 0.0219
Epoch 3/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - accuracy: 0.9894 - loss: 0.0393 - val_accuracy: 0.9975 - val_loss: 0.0181
Epoch 4/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 17ms/step - accuracy: 0.9961 - loss: 0.0160 - val_accuracy: 1.0000 - val_loss: 0.0027
Epoch 5/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - accuracy: 0.9990 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0013
Epoch 6/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 20ms/step - accuracy: 0.9964 - loss: 0.0082 - val_accuracy: 0.9850 - val_loss: 0.0342
Epoch 7/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - accuracy: 0.9965 - loss: 0.0166 - val_accuracy: 0.9975 - val_loss: 0.0087
Epoch 8/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 20ms/step - accuracy: 0.9972 - loss: 0.0227 - val_accuracy: 0.9950 - val_loss: 0.0214
Epoch 9/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9975 - val_loss: 0.0128
Epoch 10/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - accuracy: 1.0000 - loss: 2.3879e-04 - val_accuracy: 0.9975 - val_loss: 0.0156
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 128ms/step
Text: I absolutely love this!
Prediction: Positive
Text: I can't stand this product
Prediction: Negative

```


## ì „ì²´ íŠ¸ëœìŠ¤ í¬ë¨¸ ì‹¤í–‰í•˜ê¸°


```python
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, LayerNormalization, Dropout, Add, Input
from tensorflow.keras.optimizers import Adam
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# ê¹ƒí—ˆë¸Œì— ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
!git clone https://github.com/taehojo/data.git

# CSV íŒŒì¼ ë¡œë“œ
dataframe = pd.read_csv('./data/sentiment_data.csv')

# ë°ì´í„°ì™€ ë¼ë²¨ ì¶”ì¶œ
sentences = dataframe['sentence'].tolist()
labels = dataframe['label'].tolist()

# ì„ë² ë”© ë²¡í„° í¬ê¸°ì™€ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ì„¤ì •
embedding_dim = 128
max_len = 10

# í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(sentences)
sequences = tokenizer.texts_to_sequences(sentences)
word_index = tokenizer.word_index

# íŒ¨ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶¤
data = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len, padding='post')

# ë°ì´í„°ì…‹ì„ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)

# í¬ì§€ì…”ë„ ì¸ì½”ë”© í•¨ìˆ˜
def get_positional_encoding(max_len, d_model):
    pos_enc = np.zeros((max_len, d_model))
    for pos in range(max_len):
        for i in range(0, d_model, 2):
            pos_enc[pos, i] = np.sin(pos / (10000 ** (2 * i / d_model)))
            if i + 1 < d_model:
                pos_enc[pos, i + 1] = np.cos(pos / (10000 ** (2 * (i + 1) / d_model)))
    return pos_enc

# í¬ì§€ì…”ë„ ì¸ì½”ë”© ìƒì„±
positional_encoding = get_positional_encoding(max_len, embedding_dim)

# ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ë ˆì´ì–´(ë§ˆìŠ¤í¬ ì§€ì›)
class MultiHeadSelfAttentionLayer(tf.keras.layers.Layer):
    def __init__(self, num_heads, key_dim, masked=False):
        super(MultiHeadSelfAttentionLayer, self).__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
        self.norm = LayerNormalization()
        self.masked = masked

    def call(self, x):
        if self.masked:
            batch_size = tf.shape(x)[0]
            seq_len = tf.shape(x)[1]
            mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
            mask = tf.reshape(mask, (1, 1, seq_len, seq_len))
            mask = tf.tile(mask, [batch_size, 1, 1, 1])
            mask = (1-mask) * -1e9
            attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)
        else:
            attn_output = self.mha(query=x, value=x, key=x)
        attn_output = self.norm(attn_output + x)
        return attn_output

# ëª¨ë¸ ì„¤ì •
inputs = Input(shape=(max_len,))

# 1. ì„ë² ë”© ë ˆì´ì–´
embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim)
embedded_sequences = embedding_layer(inputs)

# 2. í¬ì§€ì…”ë„ ì¸ì½”ë”© ì¶”ê°€
embedded_sequences_with_positional_encoding = embedded_sequences + positional_encoding

# 3. ì²« ë²ˆì§¸ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (ë§ˆìŠ¤í¬ ì—†ìŒ)
attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim)
attention_output = attention_layer(embedded_sequences_with_positional_encoding)

# 4. ì”ì°¨ ì—°ê²°
attention_output_with_residual = Add()([embedded_sequences_with_positional_encoding, attention_output])

# 5. ë§ˆìŠ¤í¬ë“œ ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (ë§ˆìŠ¤í¬ ìˆìŒ)
masked_attention_layer = MultiHeadSelfAttentionLayer(num_heads=8, key_dim=embedding_dim, masked=True)
masked_attention_output = masked_attention_layer(attention_output_with_residual)

# 6. ì”ì°¨ ì—°ê²°
masked_attention_output_with_residual = Add()([attention_output_with_residual, masked_attention_output])

# 7. GlobalAveragePooling1D
pooled_output = GlobalAveragePooling1D()(masked_attention_output_with_residual)

# 8. í”¼ë“œ í¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
dense_layer = Dense(128, activation='relu')(pooled_output)
dropout_layer = Dropout(0.5)(dense_layer)
output_layer = Dense(1, activation='sigmoid')(dropout_layer)

model = Model(inputs=inputs, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, np.array(y_train), epochs=10, batch_size=16, validation_data=(X_val, np.array(y_val)))

# ìƒˆ ë¬¸ì¥ ì˜ˆì¸¡
sample_texts = ["I absolutely love this!", "I can't stand this product"]
sample_sequences = tokenizer.texts_to_sequences(sample_texts)
sample_data = tf.keras.preprocessing.sequence.pad_sequences(sample_sequences, maxlen=max_len, padding='post')
predictions = model.predict(sample_data)

for i, text in enumerate(sample_texts):
    print(f"Text: {text}")
    print(f"Prediction: {'Positive' if predictions[i] > 0.5 else 'Negative'}")

# ì—¬ê¸°ì„œ seq_len=4, batch_size=2ë¡œ ì˜ˆë¥¼ ë“¤ì–´ ë§ˆìŠ¤í¬ í–‰ë ¬ì„ ìƒì„±í•˜ê³  ì¶œë ¥í•´ë³´ëŠ” ì˜ˆì œ
seq_len_example = 4
batch_size_example = 2
mask_example = tf.linalg.band_part(tf.ones((seq_len_example, seq_len_example)), -1, 0)
mask_example = tf.reshape(mask_example, (1, 1, seq_len_example, seq_len_example))
mask_example = tf.tile(mask_example, [batch_size_example, 1, 1, 1])

print("\nì˜ˆì‹œ ë§ˆìŠ¤í¬ í–‰ë ¬ :")
print(mask_example.numpy()[0, 0, :, :])

mask_example = (1-mask_example) * -1e9
print("\n-âˆë¡œ ì¹˜í™˜í•œ ë§ˆìŠ¤í¬ í–‰ë ¬:")
print(mask_example.numpy()[0, 0, :, :])

```


**ì¶œë ¥ ê²°ê³¼:**


```
Epoch 1/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m7s[0m 37ms/step - accuracy: 0.5022 - loss: 1.1726 - val_accuracy: 0.9900 - val_loss: 0.0442
Epoch 2/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 34ms/step - accuracy: 0.9829 - loss: 0.0847 - val_accuracy: 0.9975 - val_loss: 0.0199
Epoch 3/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 33ms/step - accuracy: 0.9907 - loss: 0.0472 - val_accuracy: 0.9975 - val_loss: 0.0131
Epoch 4/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 29ms/step - accuracy: 0.9966 - loss: 0.0219 - val_accuracy: 0.9975 - val_loss: 0.0196
Epoch 5/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 33ms/step - accuracy: 0.9955 - loss: 0.0380 - val_accuracy: 0.9975 - val_loss: 0.0176
Epoch 6/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 33ms/step - accuracy: 0.9965 - loss: 0.0227 - val_accuracy: 0.9975 - val_loss: 0.0179
Epoch 7/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 28ms/step - accuracy: 0.9993 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 0.0057
Epoch 8/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 30ms/step - accuracy: 0.9994 - loss: 0.0115 - val_accuracy: 0.9975 - val_loss: 0.0159
Epoch 9/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 29ms/step - accuracy: 0.9989 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0011
Epoch 10/10
[1m100/100[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 31ms/step - accuracy: 0.9988 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 2.0845e-04
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 184ms/step
Text: I absolutely love this!
Prediction: Positive
Text: I can't stand this product
Prediction: Negative

ì˜ˆì‹œ ë§ˆìŠ¤í¬ í–‰ë ¬ :
[[1. 0. 0. 0.]
 [1. 1. 0. 0.]
 [1. 1. 1. 0.]
 [1. 1. 1. 1.]]

-âˆë¡œ ì¹˜í™˜í•œ ë§ˆìŠ¤í¬ í–‰ë ¬:
[[-0.e+00 -1.e+09 -1.e+09 -1.e+09]
 [-0.e+00 -0.e+00 -1.e+09 -1.e+09]
 [-0.e+00 -0.e+00 -0.e+00 -1.e+09]
 [-0.e+00 -0.e+00 -0.e+00 -0.e+00]]

```


```python

```

