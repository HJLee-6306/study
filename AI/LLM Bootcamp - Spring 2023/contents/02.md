
### **[2주차] 엔진의 이해: LLM의 작동 원리 (Foundations)**

**부제: Transformer, 언어의 구조를 학습하는 기계**

---

### **1. 강의 개요 (Lecture Overview)**

#### **학습 목표 (Learning Objectives)**

이번 주, 우리는 LLM의 심장부로 들어가 그 작동 원리를 이해합니다.

1.  **소프트웨어 2.0 이해:** 기존 프로그래밍(Software 1.0)과 머신러닝 기반 프로그래밍(Software 2.0)의 근본적인 차이점을 이해합니다.
2.  **신경망 기초 복습:** 퍼셉트론(Perceptron)에서 다층 퍼셉트론(MLP)에 이르기까지, 신경망의 기본 구성 요소를 복습하고 GPU가 왜 딥러닝 혁명을 이끌었는지 파악합니다.
3.  **Transformer 아키텍처 정복:** LLM 시대를 연 핵심 기술, **Transformer**의 세 가지 주요 구성 요소—**임베딩, 어텐션(Attention), 위치 인코딩(Positional Encoding)**—를 집중적으로 학습합니다.
4.  **주요 모델 계보 파악:** BERT, T5, GPT 등 현대 LLM의 발전에 큰 영향을 미친 대표적인 모델들의 아키텍처적 특징과 훈련 방식을 비교 분석합니다.

#### **지난주 복습 및 이번 주의 질문 (Review & Question of the Week)**

지난주 우리는 LLM이 가진 '범용성' 덕분에 간단한 지시만으로 다양한 작업을 수행할 수 있음을 확인했습니다. 하지만 LLM은 어떻게 문장의 '의미'를 이해하고, 단어들 간의 '관계'를 파악하며, 문맥에 맞는 '다음 단어'를 생성해내는 것일까요? 단순히 데이터를 많이 학습했다고만 하기에는 그 성능이 놀랍습니다.

> 단어들의 나열에 불과한 텍스트를, 컴퓨터는 어떻게 수치적 표현으로 바꾸고 그 안에서 문맥적 관계를 학습하여 인간과 유사한 언어 능력을 갖추게 되는가?

---

### **2. 소프트웨어 1.0 vs. 소프트웨어 2.0**

LLM을 이해하기 위한 첫걸음은 우리가 만드는 소프트웨어의 패러다임이 어떻게 변하고 있는지 인지하는 것입니다. 안드레이 카르파티(Andrej Karpathy)는 이를 '소프트웨어 1.0'과 '소프트웨어 2.0'으로 명쾌하게 구분했습니다.

*   **소프트웨어 1.0 (전통적 프로그래밍):**
    *   **작성자:** 인간 프로그래머.
    *   **산출물:** 명시적인 규칙과 알고리즘으로 구성된 코드 (e.g., C++, Java, Python 코드).
    *   **동작 방식:** 인간이 정의한 로직에 따라 입력을 처리하고 정해진 출력을 반환합니다. `if-else`문, `for`문 등으로 모든 경우의 수를 직접 제어합니다.
    *   **예시:** "이미지에서 '고양이'를 찾으려면, 귀 모양이 뾰족하고, 수염이 있으며, 눈이 동그란 패턴을 찾아라" 와 같은 규칙을 코드로 작성.

*   **소프트웨어 2.0 (머신러닝 프로그래밍):**
    *   **작성자:** 인간 프로그래머 + 데이터.
    *   **산출물:** 최적화 과정을 통해 찾아낸 **모델의 가중치(weights)** 또는 **파라미터(parameters)**.
    *   **동작 방식:** 인간은 목표(e.g., "고양이 이미지와 '고양이' 라벨을 최대한 일치시켜라")와 대략적인 프로그램의 골격(모델 아키텍처)만 제시합니다. 그러면 컴퓨터가 방대한 양의 데이터(고양이 이미지 수백만 장)를 보고 스스로 규칙(파라미터)을 학습합니다.
    *   **예시:** 수백만 개의 숫자(파라미터)로 이루어진 거대한 행렬. 이 숫자들이 무엇을 의미하는지 인간은 직관적으로 알 수 없습니다.

LLM은 소프트웨어 2.0의 정점에 있는 산물입니다. 우리는 "다음 단어를 예측하라"는 목표와 Transformer라는 아키텍처를 제시했고, 인터넷이라는 방대한 데이터를 통해 모델이 스스로 언어의 규칙을 학습하도록 한 것입니다.

---

### **3. 신경망의 기초와 GPU의 역할**

LLM의 기본 구성단위는 신경망(Neural Network)입니다. 이는 인간 뇌의 뉴런(Neuron)에서 영감을 얻었습니다.

#### **3.1. 퍼셉트론에서 다층 퍼셉트론으로**

*   **뉴런(Neuron):** 여러 개의 입력 신호를 받아, 특정 임계값을 넘으면 활성화(fire)되어 다음 뉴런으로 출력 신호를 보냅니다.
*   **퍼셉트론(Perceptron):** 이를 수학적으로 모델링한 것입니다.
    *   입력(x₁, x₂, ...): 여러 개의 숫자 입력.
    *   가중치(w₁, w₂, ...): 각 입력의 중요도를 나타내는 값.
    *   가중합(Weighted Sum): `Σ(wᵢ * xᵢ)`
    *   활성화 함수(Activation Function): 가중합이 임계값을 넘는지 판단하여 최종 출력(0 또는 1)을 결정. (e.g., Step Function)
*   **다층 퍼셉트론(MLP, Multi-Layer Perceptron):** 이 퍼셉트론들을 여러 층(Layer)으로 쌓은 구조입니다. 입력층, 하나 이상의 은닉층(Hidden Layer), 출력층으로 구성됩니다.
    *   **핵심:** 층을 깊게 쌓을수록, 모델은 데이터로부터 더 복잡하고 추상적인 패턴을 학습할 수 있습니다. (e.g., 첫 번째 층은 선과 점, 두 번째 층은 눈과 코, 세 번째 층은 얼굴 전체를 인식)

#### **3.2. GPU: 행렬 연산의 가속기**

신경망의 모든 연산은 본질적으로 **행렬 곱셈(Matrix Multiplication)**으로 귀결됩니다. 한 층의 모든 뉴런 값(벡터)과 다음 층으로의 모든 연결 가중치(행렬)를 곱하는 과정의 연속입니다.

*   **CPU (Central Processing Unit):** 소수의 강력한 코어로 복잡하고 순차적인 작업을 처리하는 데 특화되어 있습니다. (A few powerful cores)
*   **GPU (Graphics Processing Unit):** 수천 개의 단순한 코어로 간단하고 병렬적인 작업을 동시에 처리하는 데 특화되어 있습니다. (Thousands of simple cores)

원래 GPU는 3D 그래픽에서 화면의 수백만 픽셀 색상을 동시에 계산하기 위해 개발되었습니다. 그런데 딥러닝 연구자들이 신경망의 행렬 연산이 본질적으로 GPU가 잘하는 대규모 병렬 계산과 정확히 일치한다는 것을 발견했습니다. 이 발견이 2012년 AlexNet의 성공을 기점으로 딥러닝 혁명을 촉발시켰고, 오늘날의 LLM 훈련을 가능하게 한 물리적 기반이 되었습니다.

---

### **4. Transformer: 언어의 문맥을 이해하는 아키텍처**

2017년, 구글의 논문 **"Attention Is All You Need"**는 NLP 분야에 지각 변동을 일으켰습니다. 이전까지는 RNN(Recurrent Neural Network)이나 LSTM(Long Short-Term Memory)이 순차적인 텍스트 데이터를 처리하는 표준이었지만, Transformer는 순환 구조 없이 **'어텐션(Attention)'** 메커니즘만으로 더 뛰어난 성능을 달성했습니다.

Transformer의 핵심 아이디어 세 가지를 단계별로 이해해 봅시다.

#### **4.1. Step 1: 단어를 숫자로 - 토큰화와 임베딩 (Tokenization & Embedding)**

컴퓨터는 '고양이'라는 단어를 직접 이해하지 못합니다. 모든 것은 숫자로 처리되어야 합니다.

1.  **토큰화(Tokenization):** 문장을 더 작은 단위인 '토큰(Token)'으로 분리합니다.
    *   `"LLM is powerful"` -> `["LLM", "is", "powerful"]`
    *   GPT와 같은 현대 모델은 단어보다 더 작은 단위(e.g., 'powerful' -> 'power', 'ful')로 쪼개는 **Byte-Pair Encoding (BPE)** 같은 기법을 사용하여, 처음 보는 단어에도 대처하고 어휘 사전의 크기를 효율적으로 관리합니다.

2.  **원-핫 인코딩(One-Hot Encoding)의 한계:**
    *   가장 단순한 방법은 각 토큰에 고유 ID를 부여하고(e.g., 'cat': 30, 'kitten': 32), 해당 ID의 위치만 1이고 나머지는 모두 0인 거대한 벡터로 표현하는 것입니다.
    *   **문제점:** 'cat'과 'kitten'은 의미적으로 매우 가깝지만, 원-핫 벡터 상에서는 'cat'과 'car'만큼이나 멉니다. 즉, **단어 간의 의미적 유사성을 전혀 담지 못합니다.**

3.  **임베딩(Embedding):**
    *   이 문제를 해결하기 위해, 각 토큰을 저차원(e.g., 512, 768차원)의 **고밀도 벡터(Dense Vector)**로 변환합니다. 이것이 바로 **'임베딩 벡터'**입니다.
    *   **핵심:** 이 임베딩 벡터를 학습하는 과정에서, 의미가 비슷한 단어들은 벡터 공간상에서 가까운 위치에, 의미가 다른 단어들은 먼 위치에 배치됩니다. (e.g., `vector('king') - vector('man') + vector('woman') ≈ vector('queen')`)
    *   이제 모델은 단어들의 나열이 아닌, 의미가 담긴 벡터들의 시퀀스를 입력으로 받게 됩니다.

#### **4.2. Step 2: 문맥 파악의 핵심 - 어텐션 메커니즘 (Attention Mechanism)**

"그는 강가에서 낚싯대를 던졌다. **그것**은 은빛으로 반짝였다."

여기서 **'그것'**이 가리키는 대상은 '강가'가 아니라 '낚싯대'라는 것을 우리는 쉽게 압니다. 문장 내 다른 단어들과의 관계를 통해 '그것'의 의미가 결정되기 때문입니다. **어텐션**은 바로 이 과정을 기계적으로 구현한 것입니다.

*   **기본 아이디어:** 문장 내의 한 단어를 처리할 때, 다른 모든 단어들을 얼마나 '주목(attend)'해야 할지 가중치(Attention Score)를 계산합니다.
*   **어떻게 계산하는가? (Query, Key, Value)**
    *   모든 단어(정확히는 임베딩 벡터)는 세 가지 다른 역할을 위한 벡터로 변환됩니다: **쿼리(Query), 키(Key), 밸류(Value)**.
        *   **쿼리(Q):** "나(현재 처리 중인 단어)는 이런 정보를 찾고 있어."
        *   **키(K):** "나는 이런 정보(다른 단어)를 가지고 있어."
        *   **밸류(V):** "내가 가진 실제 정보(다른 단어)의 내용은 이거야."
    *   **계산 과정:**
        1.  현재 단어의 **Q** 벡터와 다른 모든 단어의 **K** 벡터를 내적(dot-product)하여 유사도, 즉 **어텐션 스코어**를 계산합니다.
        2.  이 스코어들을 Softmax 함수에 통과시켜 합이 1이 되는 확률 분포(가중치)로 만듭니다.
        3.  이 가중치를 각 단어의 **V** 벡터에 곱한 후 모두 더합니다.
    *   **결과:** 현재 단어는 다른 모든 단어의 정보가 '어텐션 가중치'만큼 혼합된, **문맥이 풍부하게 반영된 새로운 벡터**로 재탄생합니다.

*   **셀프-어텐션(Self-Attention):** 문장 자신 내부에서 단어들 간의 관계를 계산하므로 '셀프' 어텐션이라고 부릅니다.
*   **멀티-헤드 어텐션(Multi-Head Attention):** 이 어텐션 계산 과정을 여러 번(여러 '헤드') 병렬적으로 수행합니다. 이는 마치 한 문장을 다양한 관점(e.g., 문법적 관계, 의미적 관계 등)에서 동시에 바라보는 것과 같아서, 모델이 더 풍부한 관계를 학습할 수 있게 해줍니다.

#### **4.3. Step 3: 순서 정보 주입 - 위치 인코딩 (Positional Encoding)**

어텐션 메커니즘에는 치명적인 약점이 있습니다. "학생이 교수를 때렸다"와 "교수가 학생을 때렸다"는 전혀 다른 문장이지만, 어텐션은 단어들의 집합(bag-of-words)으로만 보기 때문에 이 둘을 구분하지 못합니다. **단어의 '순서' 정보가 없습니다.**

**위치 인코딩**은 이 문제를 해결하기 위해 각 단어의 '위치'에 대한 정보를 담은 고유한 벡터를 만들어, 원래의 임베딩 벡터에 더해주는 기법입니다.

*   sin, cos과 같은 주기 함수를 사용하여, 각 위치(0, 1, 2, ...)마다 고유하면서도 상대적인 위치 관계를 표현할 수 있는 벡터를 생성합니다.
*   이 위치 벡터를 단어 임베딩 벡터에 더함으로써, 모델은 "이 단어는 3번째 위치에 있다"는 절대적 정보와 "저 단어보다 2칸 앞에 있다"는 상대적 정보를 모두 학습할 수 있게 됩니다.

**[Transformer 블록의 전체 구조]**

> **입력 (단어 벡터 시퀀스)**
> → `위치 인코딩 더하기`
> → **멀티-헤드 어텐션** (문맥 파악)
> → `잔차 연결(Residual Connection) & 정규화(Layer Norm)`
> → **피드포워드 신경망(Feed-Forward Network)** (학습된 정보 심화)
> → `잔차 연결 & 정규화`
> → **출력 (문맥이 반영된 벡터 시퀀스)**

이 블록을 여러 개(e.g., GPT-3는 최대 96개) 깊게 쌓아 매우 복잡한 언어적 패턴을 학습합니다.

---

### **5. 현대 LLM의 계보: BERT, T5, GPT**

Transformer 아키텍처를 기반으로, 목적에 따라 다양한 모델들이 개발되었습니다.

| 모델            | 아키텍처            | 훈련 방식 (Pre-training Task)                                                                                                | 주요 특징                                                                                                                                            |
| :-------------- | :------------------ | :--------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------- |
| **BERT** (2018) | **Encoder-Only**    | **Masked Language Model (MLM):** 문장의 일부 단어를 `[MASK]` 토큰으로 가리고, 주변 문맥 전체를 이용해 맞추기.                | **양방향(Bi-directional) 문맥 이해.** 문장 전체를 보고 빈칸을 채우므로, 분류(Classification)나 개체명 인식(NER) 등 문장 '이해'가 중요한 작업에 강함. |
| **GPT** (2018~) | **Decoder-Only**    | **Causal Language Model (CLM):** 문장의 앞부분만 보고 다음 단어를 예측하기.                                                  | **단방향(Uni-directional) 문맥 생성.** 글쓰기, 요약, 챗봇 등 텍스트 '생성' 작업에 매우 뛰어남. ChatGPT의 기반.                                       |
| **T5** (2019)   | **Encoder-Decoder** | **Text-to-Text Transfer:** "Translate English to German: ...", "Summarize: ..." 와 같이 모든 작업을 텍스트 입/출력으로 통일. | **다재다능한 프레임워크.** 입력 텍스트로 '과업' 자체를 정의하는 유연함. 번역, 요약 등 입력과 출력이 명확히 구분되는 작업에 효과적.                   |

---

### **6. 2주차 정리 및 과제**

#### **핵심 요약 (Key Takeaways)**

*   LLM은 데이터를 통해 스스로 규칙을 학습하는 **소프트웨어 2.0** 패러다임의 산물입니다.
*   딥러닝의 물리적 기반은 대규모 행렬 연산을 병렬 처리하는 **GPU**에 있습니다.
*   **Transformer**는 **임베딩**으로 단어를 벡터화하고, **셀프-어텐션**으로 문맥을 파악하며, **위치 인코딩**으로 순서를 학습합니다. 이 세 가지가 LLM 능력의 비밀입니다.
*   모델의 아키텍처(Encoder-Only, Decoder-Only, Encoder-Decoder)와 훈련 방식에 따라 그 용도와 강점이 달라집니다.

#### **다음 주 예고 (Next Week's Preview)**

이제 우리는 LLM의 엔진이 어떻게 작동하는지 이해했습니다. 다음 주에는 이 강력한 엔진을 우리가 원하는 방향으로 정교하게 조종하는 기술, 즉 **프롬프트 엔지니어링**의 세계로 떠나보겠습니다.

#### **과제 (Assignment)**

1.  **[개념 확인]**
    *   어텐션 메커니즘이 기존의 RNN/LSTM에 비해 가지는 가장 큰 장점은 무엇일까요? (Hint: 병렬 처리, 장거리 의존성)
    *   BERT와 GPT의 아키텍처 및 훈련 방식의 차이점이 각각 어떤 종류의 다운스트림 태스크에 더 유리하게 작용할지 자신의 언어로 설명해 보세요.

2.  **[시각화 실습]**
    *   **Hugging Face** 라이브러리를 사용하여, 간단한 문장에 대한 어텐션 가중치를 시각화하는 코드를 작성해 보세요.
    *   예시 문장: "The animal didn't cross the street because it was too tired."
    *   BERT 모델을 로드하고, `it`이라는 토큰이 문장의 다른 어떤 토큰들(`animal`, `street` 등)에 더 높은 어텐션 점수를 주는지 확인하고 그 결과를 해석해 보세요.
    *   이 실습은 어텐션이 어떻게 대명사의 지칭 관계를 해결하는지 직관적으로 보여줄 것입니다. (필요한 코드는 웹 검색을 통해 찾을 수 있으며, 중요한 것은 결과를 해석하는 능력입니다.)