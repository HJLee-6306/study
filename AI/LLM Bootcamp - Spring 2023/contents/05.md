
### **[5주차] 중간 프로젝트: 나만의 Q&A 봇 만들기**

**부제: 아이디어를 실제 아키텍처로 구현하기**

---

### **1. 강의 개요 (Lecture Overview)**

#### **학습 목표 (Learning Objectives)**

이번 주, 우리는 이론을 넘어 실제 시스템을 구축하는 단계로 도약합니다.

1.  **실전 아키텍처 이해:** 간단한 스크립트를 넘어, 실제 서비스로 운영되는 Q&A 봇(`askFSDL`)의 전체 시스템 아키텍처를 이해합니다.
2.  **핵심 구성 요소 분석:** 데이터 수집/변환/적재(ETL), 인덱싱, 서버리스 백엔드, 사용자 인터페이스 등 프로덕션급 애플리케이션을 구성하는 각 요소의 역할과 상호작용을 파악합니다.
3.  **데이터 처리 심화 학습:** 다양한 형태(PDF, 웹사이트, 유튜브 스크립트)의 비정형 데이터를 어떻게 효과적으로 처리하고 구조화하여 RAG 시스템의 '지식 베이스'로 만드는지 구체적인 노하우를 배웁니다.
4.  **중간 프로젝트 시작:** 1~4주차에 배운 모든 지식을 종합하여, 자신만의 주제로 Q&A 봇을 기획하고 개발하는 중간 프로젝트를 시작합니다.

#### **지난주 복습 및 이번 주의 질문 (Review & Question of the Week)**

지난주 우리는 RAG(검색 증강 생성) 파이프라인을 `LangChain`을 이용해 간단한 스크립트로 구현해 보았습니다. 하지만 단일 텍스트 파일과 로컬 벡터 스토어를 사용하는 것은 '개념 증명(Proof-of-Concept)' 단계에 불과합니다. 만약 수천 개의 PDF, 계속 업데이트되는 웹사이트, 그리고 동영상 강의들로 이루어진 방대한 지식 베이스를 다뤄야 한다면 어떻게 해야 할까요?

> 하나의 파이썬 스크립트를 어떻게 여러 데이터 소스를 처리하고, 수많은 사용자의 요청에 안정적으로 응답하며, 지속적으로 확장 가능한 '서비스'로 발전시킬 수 있을까?

---

### **2. 사례 연구: `askFSDL` 봇 아키텍처 심층 분석**

`askFSDL` 봇은 Full Stack Deep Learning 강의의 모든 자료(강의 노트, 동영상 스크립트, 관련 논문 등)에 대해 질문하고 답변을 얻을 수 있는 실제 RAG 애플리케이션입니다. 이 시스템의 아키텍처를 통해 프로덕션급 RAG 시스템의 구성 요소를 배워봅시다.

#### **2.1. 전체 시스템 아키텍처 (High-Level Architecture)**

  
*(강의 자료에는 아래 내용을 시각화한 명확한 아키텍처 다이어그램을 포함)*

`askFSDL` 시스템은 크게 4개의 논리적 부분으로 나뉩니다.

1.  **데이터 소스 (Data Sources):**
    *   **다양성:** FSDL 웹사이트(HTML), 강의 자료(PDF), 유튜브 강의(동영상/스크립트) 등 다양한 형태의 원본 데이터가 존재합니다.

2.  **데이터 처리 파이프라인 (Data Processing Pipeline - ETL)**:
    *   **역할:** 원본 데이터를 가져와(Extract), RAG 시스템에 적합한 형태로 변환하고(Transform), 영구적인 저장소에 적재(Load)합니다. 이 과정은 주기적으로 실행되어 지식 베이스를 최신 상태로 유지합니다.
    *   **기술 스택 예시:** Python 스크립트, `BeautifulSoup`(웹 스크레이핑), `PyMuPDF`(PDF 파싱), `youtube-transcript-api`(유튜브 스크립트 추출).

3.  **핵심 RAG 백엔드 (Core RAG Backend)**:
    *   **역할:** 사용자 요청을 받아 실시간으로 검색 및 생성을 수행하는 부분입니다.
    *   **구성 요소:**
        *   **문서 저장소 (Document Store):** 변환된 텍스트와 메타데이터를 저장. (e.g., MongoDB)
        *   **벡터 인덱스 (Vector Index):** 텍스트 청크의 벡터를 저장하고 빠른 검색을 제공. (e.g., Pinecone, Chroma)
        *   **서버리스 API (Serverless API):** 실제 추론 로직을 담고 있는 API 엔드포인트. 사용량에 따라 자동으로 확장/축소되어 비용 효율적. (e.g., **Modal**, AWS Lambda, Google Cloud Functions)

4.  **사용자 인터페이스 (User Interface)**:
    *   **역할:** 사용자가 질문을 입력하고 답변을 받는 창구입니다.
    *   **구성 요소:**
        *   **봇 서버 (Bot Server):** Discord, Slack 등 채팅 플랫폼의 API와 통신하는 경량 서버. (e.g., `discord.py` 라이브러리로 구현된 Python 서버)
        *   **웹 데모 (Web Demo):** 빠른 테스트와 시연을 위한 웹 UI. (e.g., **Gradio**, Streamlit)

#### **2.2. 왜 이런 아키텍처를 사용하는가?**

*   **관심사 분리 (Separation of Concerns):** 무거운 **데이터 처리**(ETL)와 실시간 **사용자 요청 처리**(API)를 분리함으로써, 각 부분을 독립적으로 개발하고 확장할 수 있습니다. 데이터 파이프라인은 하루에 한 번만 돌아도 되지만, API는 1초에 수백 번 호출될 수도 있습니다.
*   **확장성 (Scalability):** 서버리스 API(`Modal` 등)를 사용하면 갑작스러운 사용자 증가에도 서버가 다운되지 않고 자동으로 대처할 수 있습니다. 사용자가 없을 때는 비용이 거의 발생하지 않습니다.
*   **유연성 (Flexibility):** 사용자 인터페이스(Discord 봇, 웹 데모)를 백엔드와 분리함으로써, 나중에 모바일 앱이나 다른 인터페이스를 추가하기 용이합니다.

---

### **3. 데이터 처리 심화: 쓰레기에서 보물로**

RAG 시스템의 성능은 "Garbage in, Garbage out" 원칙을 극명하게 따릅니다. 지식 베이스의 품질이 곧 답변의 품질입니다. `askFSDL`이 다양한 데이터 소스를 어떻게 '보물'로 바꾸는지 살펴봅시다.

#### **3.1. 웹사이트 (HTML) 처리**

*   **도전 과제:** 웹페이지에는 본문 외에도 네비게이션 바, 광고, 푸터 등 수많은 '노이즈'가 섞여 있습니다. 또한, 헤더(`<h1>`, `<h2>`), 목록(`<ul>`), 링크(`<a>`) 등 중요한 구조 정보가 있습니다.
*   **해결책:**
    1.  **구조 보존 파싱:** 단순히 텍스트만 추출하는 대신, `BeautifulSoup` 같은 라이브러리를 사용하여 마크다운(Markdown) 형식으로 변환합니다. 이렇게 하면 헤더 구조가 `#`, `##` 등으로 보존되어, 나중에 더 의미 있는 단위로 청킹(chunking)할 수 있습니다.
    2.  **메타데이터 추출:** 각 텍스트 조각의 원본 URL, 특히 특정 섹션을 가리키는 앵커 링크(`#section-title`)를 메타데이터로 함께 저장합니다. 이는 나중에 답변의 출처를 정확히 제시하는 데 사용됩니다.

#### **3.2. 유튜브 비디오 (동영상) 처리**

*   **도전 과제:** 동영상 자체는 텍스트가 아닙니다.
*   **해결책:**
    1.  **자동 자막 활용:** 유튜브는 자동으로 음성을 텍스트로 변환한 자막(transcript)을 제공합니다. `youtube-transcript-api`와 같은 라이브러리를 사용하면 이 자막을 타임스탬프와 함께 쉽게 추출할 수 있습니다.
    2.  **의미 단위로 재청킹(Re-chunking):** 유튜브 자동 자막은 보통 1~2초 단위로 매우 잘게 쪼개져 있습니다. 이대로 사용하면 문맥이 완전히 파편화됩니다. 따라서, 타임스탬프 정보를 유지하면서도 여러 자막 조각을 묶어 의미 있는 단락(e.g., 100~200 단어)으로 다시 구성하는 과정이 필수적입니다.
    3.  **링크 생성:** 저장된 타임스탬프 메타데이터를 사용하여, `https://www.youtube.com/watch?v=VIDEO_ID&t=SECONDS` 형태의 링크를 동적으로 생성할 수 있습니다. 사용자는 답변의 근거가 되는 영상의 정확한 시점으로 바로 이동할 수 있습니다.

**핵심 교훈:** 좋은 데이터 처리란, 단순히 텍스트를 긁어오는 것이 아니라, 원본 데이터의 **구조와 메타데이터를 최대한 보존**하여 RAG 시스템이 활용할 수 있도록 가공하는 과정입니다.

---

### **4. 중간 프로젝트: 나만의 Q&A 봇 기획 및 개발**

이제 여러분의 차례입니다. 4주간 배운 지식을 총동원하여 자신만의 RAG 기반 Q&A 봇을 만들어 봅시다.

#### **4.1. 프로젝트 목표 (Project Goal)**

*   자신이 선택한 주제에 대한 지식 베이스를 구축하고, 해당 지식에 기반하여 질문에 답변하는 RAG 애플리케이션의 핵심 기능을 구현한다.

#### **4.2. 프로젝트 단계 (Project Steps)**

**[1단계] 주제 선정 및 데이터 수집 (Week 5)**

*   **가장 중요한 단계입니다!** 여러분이 흥미를 느끼고, 데이터 수집이 가능한 주제를 선택하세요.
*   **아이디어 예시:**
    *   **취미 기반:** 특정 게임(e.g., 리그 오브 레전드)의 챔피언 공략 위키, 좋아하는 요리 분야의 레시피 모음, 특정 스포츠 팀의 최근 뉴스 기사 모음 등.
    *   **학습 기반:** 특정 프로그래밍 언어의 공식 문서, 관심 있는 분야의 최신 논문(PDF) 3~5개, 특정 온라인 강의의 스크립트 등.
    *   **실용 기반:** 거주 지역의 맛집 블로그 리뷰 모음, 자주 사용하는 소프트웨어의 도움말 문서 등.
*   **요구사항:** 최소 3개 이상의 문서(웹페이지, PDF, TXT 파일 등)를 수집하여 로컬 폴더에 저장하세요.

**[2단계] RAG 파이프라인 설계 및 구현 (Week 6-7)**

*   4주차에 배운 `LangChain` 코드를 기반으로, 수집한 데이터에 맞는 파이프라인을 구축합니다.
*   **데이터 로더 선택:** 수집한 데이터 형식에 맞는 `DocumentLoader`를 선택해야 합니다. (`WebBaseLoader`, `PyPDFLoader` 등)
*   **청킹 전략:** 데이터의 특성에 맞게 `TextSplitter`와 `chunk_size`를 조절해 보세요.
*   **핵심 기능 구현:** 사용자 질문을 입력받아, 관련 문서를 검색하고, LLM을 통해 답변과 출처(Source)를 함께 출력하는 파이썬 스크립트를 완성합니다.

**[3단계] 인터페이스 구축 및 데모 (Week 8)**

*   완성된 RAG 파이프라인을 `Gradio` 또는 `Streamlit`을 사용하여 간단한 웹 UI와 연결합니다.
*   사용자가 웹 브라우저를 통해 여러분의 Q&A 봇과 상호작용할 수 있도록 만듭니다.

**[4단계] 최종 발표 (Week 10)**

*   자신이 만든 Q&A 봇을 시연하고, 개발 과정에서 겪었던 어려움과 해결 과정, 그리고 배운 점을 공유합니다.

---

### **5. 5주차 정리 및 과제**

#### **핵심 요약 (Key Takeaways)**

*   프로덕션급 RAG 시스템은 **데이터 처리, 백엔드, 인터페이스**가 분리된 확장 가능한 아키텍처를 가집니다.
*   **서버리스(Serverless)** 기술은 변동성이 큰 사용자 트래픽을 비용 효율적으로 처리하는 데 매우 유용합니다.
*   성공적인 RAG의 비결은 **원본 데이터의 구조와 메타데이터를 보존**하는 정교한 데이터 처리에 있습니다.
*   이제 우리는 이론을 넘어, 실제 문제를 해결하는 **자신만의 프로젝트를 시작할 준비**가 되었습니다.

#### **과제 (Assignment)**

1.  **[프로젝트 기획]**
    *   중간 프로젝트로 만들고 싶은 **Q&A 봇의 주제를 선정**하세요.
    *   해당 주제에 대한 **데이터 소스를 최소 3개 이상 확보**하고, 어떤 방식으로 수집할지 계획을 세우세요. (e.g., "파이썬 공식 튜토리얼 웹페이지 5개를 `WebBaseLoader`로 읽어오겠다.")
    *   자신의 프로젝트 기획서를 강의 토론 포럼에 공유하고, 동료 학생 및 조교와 함께 아이디어를 발전시켜 보세요.

2.  **[기술 탐색]**
    *   `LangChain`의 **Document Loaders** 문서를 탐색해 보세요. ( [https://python.langchain.com/docs/integrations/document_loaders/](https://python.langchain.com/docs/integrations/document_loaders/) )
    *   자신이 수집할 데이터 형식(PDF, CSV, YouTube 등)에 맞는 로더가 있는지 찾아보고, 간단한 사용법 예제를 확인해 보세요.