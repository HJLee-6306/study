# Lab 07: Web Deployment (FSDL 2022)

**채널:** deep learning

**비디오 ID:** 2j6rG-4zS6w

**URL:** https://www.youtube.com/watch?v=2j6rG-4zS6w

**파싱 날짜:** 2025-07-08T17:04:39.329231


## 설명
New course announcement ✨We're teaching an in-person LLM bootcamp in the SF Bay Area on November 14, 2023. Come join us if you want to see the most up-to-dat...


## 대본

hey folks and welcome to the seventh lab in full stack deep learning on deployment this is a really exciting lab because this is when we finally really start to move away from just building machine learning models and starting to actually build a machine learning powered application so let's dive in in this lab we're going to first cover how to convert our pi torch models that we use during training into some more portable torch script binaries and then we're going to talk about how we can use a library called gradio to quickly create a user interface around our model that we can share and then we'll take it one step further splitting out our model from our front end interface and creating a longer lasting public url so we can share our model so as a sneak preview this is what we're going to end up with we're end up with a url that anyone can navigate to to try out our text recognition system submit inputs run inference and get outputs so to get the text recognizer running we need some good model weights so we trained those on a nice 8 gpu setup in the lambda labs gpu cloud and then we stored the checkpoints during training in wnb's artifact storage so we need to pull those weights down they're in this checkpoint format which is designed for restarting training but we don't necessarily care about restarting training anymore we just want to run the model so after we've reloaded that model into the lightning module just like we would if we were trying to restart training we'll call the two torch script method on it which converts the model into this torch script format it's a lot lighter weight you don't even need a python runtime in order to run a torch grip model much less all the other heavy dependencies that we have for model development like w and b and pi torch lightning and all the data science libraries that they depend on once a model has been converted to torch script then we save it in debian b's cloud storage just like we do with our model checkpoints so we wrap this entire process up into a little script stage model that does our handoff basically from training to production once it's stored you don't need to do any of those steps you just need to pull the binary file down and so that's what we'll do inside this notebook we run the stagemodel.pi script and we fetch the compiled model from where it's stored in wnb's cloud storage one advantage of using wnb not just to store information about training but also to store some information about production is that we can connect the model that's running in production to the training runs the experiments that we've logged to wnb so inside the wnb interface what that looks like we have a tracked execution in w and b that created this compiled torch script model that's production ready and if we look at that experiment and its artifacts we can see that it took in a model checkpoint and produced this production ready text recognizer and if we check out that model checkpoint we can connect back to a model training run with all of our metrics and other information that we've stored when we're only running a few experiments maybe only working with one or two versions of a model having all of this connecting metadata available in a ui or an or an api doesn't feel that important it's easy to keep track we haven't run that many 10 hour long experiments so it's not that hard to find the one that trained the model that's in production but as the team grows as the model and the application matures being able to access this information programmatically is going to become really important so there is a performance benefit to switching over to torch script but really the primary benefit here is that now our model is much more portable easy to spin up so we include a new python module paragraph text recognizer with this paragraph text recognizer class in it because this model is in the torch script format it's just one line to pull it in torch.jit.load and then we just have to take our inputs and put them into the format that the model expects using this stem component and then we take the model outputs turn them into strings and now we have an image to string model that does our text recognition this is way simpler than the way that we were creating models in training and has a much lower code footprint so with just that class we can pass in an image or a path that points to an image and get out what the handwritten text content of that image is as usual we designed this to work both as an importable python object something that you can play around with a notebook and as a simple command line interface a simple script that takes in a file name that points to a file maybe locally maybe it's a file at a url or in cloud storage and then runs the text recognizer on that input we also add a quick little test for this model development process because it's so important so critical to what we're trying to do create a machine learning application we want to make sure that changes that we make as we're iterating on the model don't break this process so let's take a quick look at how this test works our goal is to test end-to-end model development getting the data onto the machine using it to generate gradients update parameters save that model convert that model to torch script and then upload that model to wnb and then pull it back down so all of these steps are run in this script and if any of them error out then we'll fail one important difference between what's happening in this lab and what's happened in previous labs is that we actually are no longer using the gpu so if you're running on coab and you check which run time type you have you'll find that you don't have a gpu accelerator and that's because we don't necessarily need to have gpus to accelerate our models once they're running inference even if they're necessary during training so we go through a couple of the reasons in this section of the lab one that i wanted to just walk through in this video very quickly is that batching is actually really important to what makes gpus efficient during training so when our model is running in production users are going to be sending requests and they're all acting independently of each other so we don't control when data comes in that we want to run inference on very much unlike during training and so the simplest way to run our model is to do inference on a single input example at a time but this is really terrible for gpu utilization gpus get more efficient and more effective the more things they can do in parallel and it's really easy to parallelize across a batch and so having large batches really makes gpus much more efficient so we link to a wnb report where you can compare side by side two traces from from a few steps of our text recognizer model running a batch size of 16 versus running on a batch size of one but in this video i just wanted to look at the high level profile summary from tensorboard for these two so the batch size 16 example this looks a lot like what we saw in the performance troubleshooting lab where we saw super high gpu utilization which means that almost all times during the model's execution something is happening on the gpu this is our baseline metric that we optimize to make sure that we're making good use of our of those expensive gpu accelerators if we look at the high level summary for the model running on a single input at a time on batch size one we see that our gpu utilization is tanked it's only 38 instead of above 90 we can see that for a large fraction of the time what we're waiting on is actually things happening on the cpu we're bottlenecking almost as much on picking which kernels to run as we are on on running those kernels on the gpu well the details here might differ from application to application it's always going to be the case that getting a lot out of your gpus in deployment is much more challenging than getting a lot out of them during training so you should only reach for gpu back deployment if you've tried cpu back deployment and found that it doesn't work in your use case so we've made our model more portable we've created a command line interface for our models so it can run on input files that we pointed to no need to worry about pi torch data sets or data loaders but a command line interface is not really a good user interface for a deep learning model deep learning models often operate on the types of data that humans really care about audio text images and for the same reasons that we like to use jupyter notebooks so that we can see and play around with this rich data while we're developing our models we also want to be able to see and play around with this rich data while we are using our models and to double down on a point from lecture having a user interface like this available as early as possible during development is really important for designing high quality models this is closer to the way your users are going to interact with your model they're not going to be creating pi torch data sets and sending batches of inferences and calculating metrics on them they're going to be sending data that matters to them from their phone from their computer directly to your model and looking at one prediction at a time and the closer that we can get to our user experience while we're doing our development the better that user experience is going to be so we've added a new python module appgradio that builds a basic user interface around our model using a library called gradio radio makes it really really easy to wrap a simple user interface around a single python function that takes in some inputs and produces some outputs this is enough to get a minimum viable product for actually a pretty large number of ml powered applications after all the core machine learning models that we're training are functions that take in some input and return some output so let's take a look at how we build our inner user interface with gradio so the core component is down here at the bottom where we generate this radio interface we pass in a python function that we want to be able to interact with through this user interface so that's going to be like the predict method of our tax recognizer and then we also say what kind of inputs and what kind of outputs does this model have that makes it possible for gradio to create simple widgets for users to provide inputs for the model and in order to display the model's outputs so we have image inputs and text outputs here that's really the core of it the rest of these things are mostly sort of presentation so giving it a nice title including a description and a readme and adding some example inputs so that users can see the kinds of things we expect them to submit once we have that gradio interface object we can create a user interface that we can interact with just by running this launch method and one of the nice things about gradio is that it's designed to be able to run in a notebook just as easily as it can run from the command line so we could take our uh appgradio slash app.pi script run it there and we'd have this interface running from the command line we can also take a look at it inside this notebook without having to leave but we're not just running this inside the notebook this is not just something that we have only inside of our special computational environment here this is something that anybody can interact with the model is running inside of our collab notebook or on our local machine where we're running jupyter but anyone can use this interface just by going to the url that's been printed to the standard output so let's check that out so here's our nice user interface where we can drag and drop images upload images or choose one of these examples and then submit them we can even edit them with a simple image editor here and then click submit to get the model's inference on them not so bad when you run this notebook and you get that url try going to it from another device maybe your phone or another computer and play around with the model at the same time as we create this ui this user interface we also get an api an application programming interface something that we can interact with programmatically to send data via requests and get responses so clicking the show api button at the bottom of your gradle interface will give you the url for the api and the format that it expects so the general format here is json javascript object notation which is basically a generic standard for representing dictionaries across languages so we show a quick demonstration here back in the notebook of how you might interact with this api via the command line so the curl tool is a simple unix command line tool that can send json formatted data among other things to apis and collect the responses the one tricky bit is how do we encode our images in order to send them in our request to the model so in these http based rest apis the most common way of formatting binary data when we're communicating it over the network is in so-called base64 format which uses numbers and letters from the ascii character set to represent the binary data in base 64. so we don't have to do this ourselves there's things built into both unix and into languages like python to handle this but it is something that we have to take into account so in the first line we take one of our images and we encode it in base64 wrap it up into the json format that our api expects and then we send it to the model with curl once again we've added a bunch of new functionality and this is something that we want to make sure that we're not breaking as we're adding new features changing things around so we again add a simple quick test thoroughly testing web applications is really challenging it's probably going to require tools outside of the python toolkit and especially as you're iterating quickly and developing your model these tests are going to be more trouble than they're worth so we just do the most simple test we just check that the things we just did in the notebook creating the user interface and then pinging the api run without error so we create our frontend based off of our paragraph text recognizer and then we send a request and check that there was no error and that there was that there's some data in the response so the python function that that gradio interface was wrapping was just our paragraph text recognizers predict function so that means when we spin up the server we need to create the model and then the same process that is serving that user interface and responding to user clicks and interactions is also the one that is running the model and this is perfectly fine when you're first getting started it's super straightforward this is the model in server architecture that we saw in the deployment lecture but pretty quickly we're going to want we're going to want to separate out our model back end from our user interface front end and provide a model as a service so a model service architecture there's lots of ways to do this the simplest way that's compatible with easily scaling up your model serving without having an entire infrastructure team is to use serverless cloud functions that are provided by all the major cloud providers so unlike servers that are up 24 7 and that maintain a lot and manage a lot of state in user sessions serverless cloud functions are only running when they're needed so the serverless tool for amazon web services is called aws lambda and the python code that goes into that serverless cloud function is in this api serverless new module in lab 7 so let's take a look at that so once again we're creating that paragraph text recognizer class which is fundamentally based on the torch script version of our model and then we write a handler function that wraps around that model and its predicts function so the predict function is expecting to receive file names for images or images but these cloud functions communicate via json blobs and so we need to take the json blob that comes in this event and pull the image out of it and then send that image to the model.predict function we've also got a little bit of logging code here that prints information about what's going on inside our function this will get automatically collected for us in aws once we've got the model's outputs the model outputs a string we've got to package that back up into something that aws can turn into json so we package that up into a dictionary and that's what we finally return and then part of what is provided by aws lambda is converting that into a proper http response that a tool like curl or a browser can understand so setting up a serverless function requires an aws account and setting up your credentials and configuration and can cost money so rather than setting one up inside the lab we just show you what it looks like to talk to one that's already been set up that's running on the full stack deep learning aws account so as of this year this actually got a lot simpler it used to be the case that aws lambdas could only be talked to via aws but now they all come with a url which means we can talk to them like we talked to any other web service we can send a request directly so when we were talking to the gradio api we did it via the command line using unix tools in this cell we demonstrate what it looks like to use the python request library which is a much more ergonomic and easy to use way to write these kinds of http requests and handle the responses and we can just use this we don't need any special aws specific tools or anything and so this cell demonstrates that we can send an image to this serverless cloud function and a few seconds later get back the model's inference of what text is contained in it so our big win here is that we no longer are running the server and the model in the same place on the same hardware so we can develop those two things independently of each other so for example we can run a gradio app locally here in this jupiter notebook which might be on collab which might be on your own machine but then run the model on aws infrastructure we just swap out our predictor back end no longer using the paragraph text recognizer class but instead pointing it to a url and most of the work on this is done using this predict from endpoint method of that predictor backend class let's take a look at that so this basically does the exact same thing as that cell we were just looking at getting an image ready to be posted as an http request and then pulling the prediction out of the response so now we call make frontend but the function that we're wrapping instead of being the whole model.predict is instead just this request posting and the end result isn't something that's going to look different as we interact with it and play with it inside the notebook but you'll notice if you are on your own machine and take a look at the resource consumption that the model is no longer running on the same machine as this user interface so we've spun up a serverless model service and we've created a ui for users to interact with that model service so we're almost done with setting up a reasonably professional ml powered application or at least the minimum viable reasonably professional ml powered application that we can slowly iterate on scale out replace pieces and eventually end up with something really high quality the one missing piece is that the url for sharing this model uh is the url controlled by gradio so you'll notice they have a five digit number in front and then doc radio.app and you may have also seen a warning that that url is only good for 72 hours so what we need to do to finish this process is set up our own public urls and no longer rely on gradio to provide them for us so importantly gradio is still doing about half of the work for us here radio creates a local url that we can talk to from the same machine so this local url will have an ip address like 127.0.0.1 which is the ip address of the current machine depending on your configuration it might say localhost instead unlike other ip addresses which point to specific machines this always means whatever machine this code is running on right now and so if we run tools like curl or a browser on the same machine and we point them at that specific url then we can interact with our model our user interface and our api but running it on some other machine will give us an error so fundamentally what we need to do is take this service that is running on an ipv that's only available locally and make it available globally and doing that right is a little bit tricky one of the trickiest but also most important bits is that you'll want to use encrypted communication https instead of http and setting that up on your own can be kind of a headache so we'll do what we did when we wanted to make label studio accessible via a public url we'll use ngrok and the free tier of ngrok includes both public urls and secure secure communication with https the biggest downside is that that url will change every time you restart your service maybe after an outage and there's effectively now not really a good way to get a free static public url but this is something that you can either pay for through a cloud provider or pay for ngrok or a similar service so that you can have that branded static url for your application so once you've logged into ngrok it's as simple as just connecting an ngrok tunnel to that port at the end of our local url and then you can head to that ngrok.iourl and you can share that url with others and they can interact with your model we've done all this by the way in the jupiter notebook just because that's the easiest way to combine this code with explanations and visualizations but all this can absolutely be done from the command line that's the right way to do it when you're running a web service running a web server out of a jupyter notebook is kind of crazy and so these two commands down here show you how to do that from the command line if you want in addition to not running the server from a notebook we probably also don't want to run it from our development machine and we certainly don't want to run it from colab which shuts off automatically after a few hours or days so we want instead a dedicated server for this application and the simplest way to do this is to spin up a machine in a cloud provider so elastic compute cloud aka ec2 is the option for doing this in aws so then we need to replicate a bunch of stuff that has happened in this notebook on that machine we need to get cloned to pull down the library we need to install the production requirements fetch a compiled model with that stage model script and then finally run app.pi and ngrok to create the user interface and then create a public url for it and as you're first getting started that's totally fine manually setting up your own server doesn't take that long but once you start responding to outages or working with more people in a team you're going to want to automate this process of setup and simplify the management of all these requirements and how to execute all of these commands with which arguments and in which environment and so the right way to do that is by creating a container that essentially automates all those steps that i described we provide an example docker file that can create a docker container that can run our model frontend either with the model running inside the server or with the model running somewhere else maybe serverlessly we can't build container images or run containers inside of colab so we won't actually use this docker file in the lab but let's go through it really quickly to get a flavor for how containers are built docker files are written in this domain specific language that has a couple of simple verbs in it or commands like from or run with each of them on a different line so the first line from says which container image we're starting from and this usually is going to have at least the operating system that you're basing off of whichever linux version you're using often it'll include things like gpu drivers if you're doing gpu based inference here it's just got python 3.7 in it then we run a command to create a working directory called slash repo into which we can put all of the code that we need and so then we copy our first piece of information from the machine we're using to set up our docker container into our docker container with this copy command we bring in the requirements file for production and then we run pip install to install them inside this container so one tip for writing docker files you want to put all of your heaviest slowest stuff as early as possible so things like installing dependencies and environments because each line of a docker file is cached independently these are the layers of your container image and if you change a layer halfway down when you rebuild your container rather than starting from scratch you'll start from halfway down the bottom layers the ones that come first in the docker file should be the ones that change the slowest and take the longest to run once we've got our requirements we start copying over the things that we're going to need so our text recognizer code our gradio app code and any configuration information note that we don't copy over our training code because we don't need to do that in production and we can also use this dot docker ignore file kind of like a dot git ignore file to say which files we don't want to bring over we don't want to copy over and then finally we define what happens when you call docker run that's this entry point command here at the bottom we'll call python on appgradio slash app.pi and set a fixed port for the server to run on always a good idea to document your entry points and how you expect them to be used inside the docker file note also that any other things that get included in the docker run command will also get passed to that original python script so for example you can include dash dash help and you'll get the python script help printed in the container so what you've called docker build on this docker file somewhere you can then push that container image to a container registry a place like docker hub kind of the github for docker containers or if you want a little bit of more control over access to a container registry inside your cloud service and then you can set up just by pulling the image down to machine and running docker run so no need to worry about setting up environments making sure they don't conflict they don't conflict with anything else running on that server or worrying about any of the configuration that's inside that entry point so to recap we took our pie torch model and compiled it down to torch script so that we could make it more portable run in different places we wrapped a gradient ui around that torch script model so that we could play with it interactively via a graphical interface and then we separated out that graphical interface from the model execution so that we'd be ready to scale them independently of each other and that's the basic process for setting up a model service architecture for an ml-powered application so this workflow is enough for you to start making your own mo powered applications on the basis of your own models in the final lab in this series we'll look at how to monitor and debug models that we've deployed and start to close the loop and get to the point where we can use what we've learned from our users interacting with our model in production to make a better model i'm looking forward to it and i'll see you there
