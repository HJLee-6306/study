# Lab 05: Troubleshooting & Testing (FSDL 2022)

**채널:** deep learning

**비디오 ID:** D65SlCSoS-0

**URL:** https://www.youtube.com/watch?v=D65SlCSoS-0

**파싱 날짜:** 2025-07-08T17:04:26.363471


## 설명
New course announcement ✨We're teaching an in-person LLM bootcamp in the SF Bay Area on November 14, 2023. Come join us if you want to see the most up-to-dat...


## 대본

hey there folks charles here for full stack deep learning today we're going through the fifth lab which talks about testing machine learning systems and troubleshooting deep learning models for performance as always with the labs you can find a link to the notebook on colab on the github repository or you can clone it from there and run it locally you can also find links in the description of this video so first we're going to cover practices and tools for testing and linting python code in general and then a little bit about some tests for ml training systems in particular and then we'll spend some time going through what the training step for a neural network in pytorch looks like tracing it and looking at the actual execution and using that to understand how we can optimize the performance of our models and so troubleshoot issues with models that run too slowly so while linting and code cleaning is important we want to make it as automated as possible and towards that purpose we bundle up all of our linting and style tracking tools into a single command using a tool called pre-commit so we can see that we've run it here we've run this command line tool from the notebook to check all of our files one of the really useful things about pre-commit is that it installs all of the dependencies for tools in separate environments so you don't have to worry about making sure your development environment for your models is also compatible with the expectations and demands of your linting tools which can sometimes be mutually exclusive even so on the first run you'll find that you have to wait a few minutes for all these things to be installed but then after that it's quite quick nice so it only took a few seconds and that's definitely something you'll want to aim for with these pre-commit checks these are things that are run on every git commit and so that can be pretty often in certain workflows so this list of outputs here is a list of all the checks that are run and whether they passed or failed so they're all green so that means that they all passed so some of these checks include just basic hygiene things like not committing in the middle of a merge conflict they also include some of our python specific things they're configured via yaml file like a lot of these tools like the automation tool github actions or our hyperparameter sweeping tool wnb sweeps pretty common file for configuring these things and the top part here includes basic pre-commit hooks that check things like the formats of our yaml files check whether we're accidentally leaking a private key and then below that are our more specialized tools are automatic formatting tool black are python style linter and checker flake 8 and its extensions and shell check for linting our scripting files flick 8 is a tool that also needs configuration one of the most important reasons to get good at configuring these tools is to ensure that you always have an escape valve so if you just enforce these tools in a simple way every component of the style guide is applied and it's applied to every line in every single file you're gonna find yourself fighting against these linting tools rather than using them to help you write code more quickly and more effectively so it's important to be able to do things like this large block in the middle of extend ignores that say i'm going to ignore all of these checks because i don't want to apply them right now so for example we actually turn off the flake8 annotations tool for now because we aren't keeping our type annotations fully in line with that style guide something to ship in a future version in addition to linting and checking our python files and some of our data files it's also really important to check all of our shell scripts because these are a very common source of bugs shell scripts often need to do something pretty simple but in part because they're older tools and we've learned a lot about how to build tools since then and in part because they come from a time in which engineers kept manuals for their tools and learned them very thoroughly before using them there's a lot of behavior that's quite surprising to the contemporary engineer and unexpected and so that makes these just a really common source of bugs there's a couple ways to help with this one of them is to have a linting and checking tool so shell check is the tool we recommend shell check is incorporated into our precommit here so it doesn't have to be run separately but it's such a useful tool that i think it's a good idea to incorporate into your editor your editor should have support for running pre-commit on files as well so you can run some of your pre-commit hooks while editing and shell checks super fast so it doesn't break your editing workflow i would recommend dropping in some scripting files maybe from your own machine from your own projects or from projects that you like from a repository like github and seeing just how many subtle bugs there can be due to things like files with new lines and spaces in their names or surprising behaviors of arrays or variables in bash scripts one of our other suggestions is to slightly change the default settings of bash when you're writing a script so that there are more loud failures one of the biggest issues in machine learning and trying to test machine learning is that failures are silent they're not caught directly the system keeps running even though it's not working this is also a feature of bash shells because in general you don't want an error to just kick you out of your login session require you to log back in that's a bad experience but when you're writing a script you actually do want failures to cause the shell to terminate that script is not running in a login shell it's running underneath that login shell so we want to surface that error to the shell that's running our script there's a brief description of these ideas here you can also check the original blog post on unofficial bash strict mode by red symbol so in addition to linting and checking some things about the style of our code we also want to check it for correctness and so the basic tool for this is writing tests of our code writing code that fails when other code has issues even if those other issues might be silent otherwise so this might start off when you're rapidly developing stuff as just throwing assert statements into your code so assert statements raise an error if the value provided to them is false if the value provided to them is true they keep running and that's a decent start especially when you're moving really quickly and developing maybe even inside of a notebook while you're trying out some functionality but it has some limitations one of the biggest ones is these things are going to be scattered throughout your code base and hard to find so it's better to collect these up into specific functions that are easily identifiable as testing functions or testing classes and then hook those up into testing tools so the standard tool for testing python code is called pi test and if you pass it pi test a file name it'll look for any classes to serve a test any functions that start with test underscore and run them and report the results so for this simple test of our character error rate calculating function we get both an indication that our tasks pass and also this report of coverage which lines in the code base were executed during this test this coverage report comes from an additional tool called codecov that's nicely compatible with pi tests and actually also other testing tools for multi-language code bases so that test character error rate function was located inside the same file as our implementation of character error rate that's another good workflow for when you're moving really quickly but it can make it hard for tests to be discovered both by automated tools and by other engineers so it's better practice to start collecting up your tests at the level of a module or the level of a project into a tests folder so it's easy to see at a glance what all the testing files are and name them clearly so it's obvious what they test and in this section of the lab we walk through the design of a specific test for an error handling component of our custom pytorch lightning callbacks and explains why we chose to structure the tests in the way that we did and also which kinds of tests we chose not to implement which is at least as important as the tests that we do choose a sculpture is defined not just by the rocks that are there but by the rocks that are not there in addition to testing the code in our implementations it's also a good idea to test the code in our docs strings and additionally we want code in our docs string so that people looking into our methods and our classes know what kinds of behaviors to expect of course without testing that code is susceptible to rot just like the rest of our implementations we want to use this module in the python standard library doctest incorporated with pi test and run it so you can see right above my head there's some python code in this docs string that looks like a session in a python interpreter with doctest we actually can execute that code and check that the outputs are as expected you may even have seen some of these code snippets in libraries that you've used like numpy and not realized that these were actually executable and tested bits of code so far we've mostly been talking about general practices for testing python projects in general and nothing really specific to machine learning in the lab we go through a little bit on basic ideas around testing data and data handling code we go into a little bit more detail about tests for training and one of the most important kinds of tests for training are memorization tests tests that check whether a model is capable of memorizing a small amount of data also known as the overfit a single batch test so here's what one of those tests look like the most important bit is there in the middle where we invoke our run experiment script with a specific set of arguments that cause it to run on the actual data that we want to train on but to only look at a single batch in each epoch so that's using the overfit batches command line argument for the pi torch lightning trainer pie charge lighting also makes basic testing of our training script easier with flags like fast devrun that are discussed in another section of the lab it can be difficult to incorporate these tests into your continuous integration because most cloud services either don't provide gpu acceleration or provide it only at a very high cost the cheap and dirty solution is to set up a regularly running job on a development machine but you want to take care to make sure it doesn't interrupt any other work you also want to make sure that this test runs as quickly as possible so it's as feasible as possible to run it on the cheapest infrastructure that you can one way to have a really nice slider that you can use to decide how long the tests run is to set the number of epochs that this test run for how many times is this single batch of data presented to the network and then set based off of empirical observation what value of the loss you would expect to get within that number of epochs and that gives you a hierarchy of memorization tests that run from maybe five minutes on a commodity gpu all the way up to a whole hour on really nice a100s then you can titrate the running of those tests to your budget and your needs you could run one of these memorization tests here in this lab notebook just by flipping the running memorization flag from false to true it should take about 10 minutes to run on typical hobbyist hardware and you can take a look at the results in the weights biases dashboard that gets generated tests are great tests are important but they are only half the battle when a test fails that's just the beginning of the story a story that doesn't end until we've resolved the issue that caused the test to fail and one of the trickiest things to resolve with deep neural networks is performance in the sense of speed latency throughput but resolving these issues is really important the faster you can train models the faster you will learn about the task about the model architectures and about how to solve your problem and the more performant your code is the larger scale you can operate at simply ignoring the complexities of problems and scaling your way out of your issues has turned out to be a pretty winning strategy in the world of deep neural networks troubleshooting the performance of deep neural networks can be challenging but there's some often low hanging fruit that you can find if you know how to run profiling and tracing of your network training code to look for the obvious simple bottlenecks that arise from some of the foot guns that are handed to you by your training framework so this section of the lab has two components we run model training for just a single epoch with profiling turned on we've added profiling to the code base this time around using some features of pytorch and pytorch lightning and then we take a look at that profiling data in two different ways first at a high level just looking at some of the summary statistics of what happened when our model was training to see what things we should be looking for to notice these basic bottlenecks that we can get around and then we take a much deeper dive into a trace of our training steps execution which lets us know everything that happened on the cpu and gpu during a couple of training steps and this trace viewer is a really unbeatable tool both for optimizing your code and finding bottlenecks and also for gaining a better understanding of just what's happening while your model is training so we'll go through it in detail because that level of understanding of your model training is really important for being able to troubleshoot performance with no extra tooling just using the profiling built into lightning and nothing else you'll get a summary table of results with practice you can make use of this summary table and draw insights from it but it's not really the best way of viewing information there's a much friendlier view of some of the same information available via tensorboard and our training script saves the required profiling information two weights and biases and weights biases has integration with tensorboard which allows us to actually see it inside their interface so let's check that out so there's a couple different views of the information that's been saved by the profiler you can use this drop-down menu to switch between them we're going to focus on this overview tab which is the place where you can go to get a quick sense of what's going on during your training step so there's some configuration information about our system letting us know what kinds of accelerators we are using how many of them we're using and some basic information about them there's documentation in the lab and links to additional documentation about what each of these things mean the most important metric that we're going to look at is gpu utilization so gpu utilization tells you what fraction of time there is actual work going on on your gpu and this is a number that we want to get as high as possible 90 or higher the gpus are the most expensive component of our training system and they're the component doing the work that is otherwise hardest to optimize so we want to make sure that that is our bottleneck optimize code has the bottleneck on the hardest piece to optimize so the goal then is for that pie chart in the top right that tells us which aspect of our training step is the bottleneck to be on kernel aka executing operations on the gpu as much as possible so we'd love to see just a blue doughnut in the top right saying that it's a hundred percent gpu utilization or 99.9 gpu utilization and other things like copying memory to and from the gpu loading data or otherwise running things on the cpu take a little much smaller fraction in the exercises we'll suggest some changes that you can do to run less optimized versions of training and see the difference easy to miss down at the bottom there's a performance recommendation section you'll need to scroll down to see it on a lot of screens if there's an obvious issue with your code maybe you're not using multi-processing for data loading maybe your gpu utilization numbers are really low there will be some suggestions here on ways to resolve that bottleneck this code has pretty high gpu utilization and even high stream machine efficiency so there's no performance recommendations here but some of the most common low-hanging fruits will be suggested here so always make sure to check that but it won't be able to catch everything and some of the suggestions are fairly general the only way to know what to do in your specific case to improve the performance of your pi torch code is to look at the details of what occurred identify the bottlenecks and resolve them for that i like to use the trace viewer which is based on a trace viewing tool originally developed for the chromium browser because we've saved our information to weights and biases we can see it inside their interface just like we can see information about our models information about inputs and outputs and other charts we can display that inside of a notebook if we'd like but when viewing stuff full screen and doing a ton of interaction it's nicer to look at it inside the wnb interface so let's go check that out so here's our trace this trace has all the operations that were executed on the cpu and the gpu laid out in time moving from left to right during a couple of training steps on the top here we have operations on the cpu and on the bottom we have the operations on the gpu it can be overwhelming at first sight there's a ton of information here it's honestly a little bit scary to see just how much happens when we do something simple like training step so the lab walks you through how to orient yourself in this and find some of the familiar pieces of neural network training forwards passes backwards passes parameter updates and i'll walk you through that visually now and you can see detailed text instructions in the lab so the top half here is our cpu execution and we can see these stacks of colored bars and this stack of colored bars is a call stack the top level is the highest level method and inside of that method other methods get called so at the very top clicking here and pulling up information we see something like optimizer step a high level part of our lightning interface as we click in we get more detail we see that we're calling the atom optimizers step which itself is calling multiplication square root division operations the things that are part of the atom algorithm for computing weight updates if you want to find something familiar in this trace highlight it there's a search bar in the top right so you can type in names of methods or classes or components and see what pops up so we're using our resnet transformer here so it should be a resnet there should be a transformer there should be encoder and decoder methods we can type all those into the search bar and see what happens so typing in resnet highlights these components you can see they're a little bit higher up vertically because resnet is a very high level of abstraction if we're more specific and search for something like convolution we'll start to see things that are lower down in the call stack more specific method we can find the beginning and ending of our training step by looking for resnet which is the beginning the encoding of our input images and then the transformer which is the second step of our forward pass decoding the images out into text we've now highlighted the transformer component and we can zoom in to a portion of the trace to see the forward's pass so to zoom in to pan around and otherwise look around inside this trace we use these floating tools here we can move them around take them put them out of the way so let's zoom in on that forwards pass here we can see our resnet forwards pass and our transformer forwards pass here we can identify the end of the forwards pass there's our soft max that calculates our probability distribution from our logits this area here that we've zoomed in on is our forwards pass if we pan to the right and look below into this separate thread we can see the backwards pass the backwards pass is always pretty easy to identify for one it happens in a separate thread from everything else and two you can just type the word backward into the search bar to highlight it so there's our backwards pass if we look at the beginning of our backwards pass in more detail we can see that it starts with calculating the loss and if we scroll to the right through the progress of the backwards pass we can see that it ends with a convolution operation so the first convolution in our resnet so it is indeed moving backwards through our network returning to our forwards pass let's take a look at the very beginning and see if we can understand what the trace is showing us about what is happening during our training step with the goal of building a better mental model for our pi torch code that will help us identify performance bottlenecks since we're focusing on the forwards pass and at the bottom the activity on the gpu let's hide the details of the backwards pass by clicking this button so if we search for data we can find the end of our data loading here and then we can see the first operation on the gpu for our forwards pass here with this mem copy from the cpu to the gpu so from the host to the device the most important thing to know about neural network code that is accelerated by gpus is that activity happens asynchronously between the cpu and the gpu while some components of the cpu are working hard to get information about the batch from the cpu's memory over to the gpu's memory the main python process is continuing to move through the computation graph you defined in your module to figure out what operation it's going to want to apply to that data once it's on the gpu we can see this information here after the copy is finished a kernel is launched an operation written effectively in c to run quickly on the gpu is kicked off the names for these are pretty complicated some of the information in them is not documented at all because it's proprietary has to do which with which fast matrix algorithms are incorporated into nvidia's libraries you can pull out sometimes basic information about whether it's a convolution or a matrix multiplication or some special neural network operation like dropout from the name what's more important is to connect it back to the high level operation in python and in pytorch that we wrote that kicked off this kernel so we can click highlight this incoming flow to see which cpu operations led to that gpu kernel being executed so let's go a little further find one of the longer running operations click to see its flow and it looks like it's coming from our com2d pytorch module so in general with these flow events what you want to see is that the cpu is deciding what work to do picking out which kernels to run long before they're actually executed on the cpu and the reason why is that the gpu can never get ahead of what the cpu is doing the cpu is in the driver's seat but the cpu is not doing the hard work the gpu is doing the hard work and it's also more expensive so we never want our expensive hard-working fast accelerator accelerator to be waiting on its slower moving host so the way that we can tell that the gpu right here is not waiting on the cpu is that there are no or very few areas in this gpu stream that are gray that are the color of the background we see colored blocks indicating that kernels are being executed on the gpu so this is synonymous with the gpu utilization metric that we see reported in the systems tab on wnb that we see when we run nvidia smi from the command line or that's reported in the top level of the pytorch profiler tensorboard integration for an example of slightly worse gpu utilization where the gpu is waiting on the host check out the atom step component by searching in the search bar you'll see on the bottom there there's lots of tiny colored bars indicating that kernels are running for short periods interspersed with large areas of gray so this is a potential target for optimization if we want to make things run faster unfortunately solutions to this are not a stable component of pytorch at the time of this recording and so there's not a simple plug-and-play replacement to resolve this issue in addition to visually assessing the gpu stream to see where there are blocks of color and where there are blocks of gray we can also use the search to find moments of synchronization between the cpu and the gpu where the cpu realizes that it needs the results of work from the gpu in order to decide what to do next it must then wait for the gpu to finish report that it's done sometimes transfer information back to the host and then during that time the gpu will sit idle so an example of this occurs for our resmet transformer in the handoff between the resnet component and the transformer component essentially because we're asking for more detailed type information than the cpu has available without waiting for the gpu to finish calculating so this is another potential target for optimization but given that we're at 90 gpu utilization we're already doing well enough on this hardware that it's probably better to think about looking into faster gpus that could crunch through this these same operations in less time rather than making possibly brittle changes to our code that will break if the problem definition changes in pursuit of small amounts of improved performance on top of walking through this tour in the lab notebook we also pull out some additional suggestions for the first things to try when you need to troubleshoot your model one piece of really great advice that i originally got from some of the folks at openai is to only focus on optimizing your forwards pass not your backwards pass the backwards pass is dependent on what happens in the forwards pass so optimizations to one will generally improve the other but the forward's pass is easier to reason about and more under our control in addition the forwards pass is the trickier bit this is the part where pi torch is building up the computation graph for all the operations that you want to do once that's done and you ask for gradients the compute graph is fixed and so pi torch is much more able to do optimizations and operate more quickly so in a sense the backwards pass takes care of itself another general suggestion you'll hear is to make your batches as large as possible while still fitting in your gpu ram so at all this time spent in python whole milliseconds to look up which kernel to use for this particular input is more easily dwarfed by the time it takes to actually perform those operations if it's on a matrix with millions even billions of entries that might take milliseconds seconds to execute on the gpu if it's a matrix with only a few entries in it it'll execute in nanoseconds much much faster than python can even finish the function execution that kicked off that gpu work so hopefully this tour in the lab and the video have helped you build better intuition for the really unintuitive features of pytorch code that make optimization so difficult most folks building a deep learning application across the entire stack won't become an expert in this kind of performance optimization because you can't be an expert in every piece of the stack but understanding the execution model a bit better and being able to read traces will help you both write pytorch code that doesn't hit any of these sharp edges and resolve them when they arise we close out with some suggestions for how to reach really high gpu utilization getting a nicer cpu than you might expect that you need looking into data loader best practices to avoid bottlenecking and other basic things that you can do that are the highest leverage ways to make sure you're not leaving a ton of potential performance on the table or spending dollars per hour for gpus to sit idle in a way that could be resolved with a change to a single line of python in my experience i found that a quick audit like this can identify those kinds of lines and give speedups from 20 to 50 so we've just got two quick exercises here i would suggest the most important exercise is to just spend some time with that trace if you're running on your own hardware you might want to compare and contrast with collab to see what kinds of differences you can observe we've selected arguments and designed our model to work well on the cards and setups that we use but you might find different settings optimize it for the setup that you have if you do we'd love to hear about it make a wnb report out of it share it with us on twitter post it here on youtube and we'll check it out we also suggest that you compare what it looks like when you turn off multi-processing for data loading setting num workers to zero instead of the default value that we select zero is actually pi torch's default value and you'll see that it leads to very low gpu utilization in almost all circumstances we also have a quick exercise so you can try out these linting tools and write yourself a quick test so that's everything for the troubleshooting and testing lab covering some of the basics of the lowest hanging fruit and the highest leverage things to tackle in making sure your networks train fast run correctly and have clean maintainable code thanks for watching post any questions you have in the comments and happy troubleshooting
