# Lab 04: Experiment Management (FSDL 2022)

**채널:** deep learning

**비디오 ID:** NEGDJuINE9E

**URL:** https://www.youtube.com/watch?v=NEGDJuINE9E

**파싱 날짜:** 2025-07-08T17:04:20.446796


## 설명
New course announcement ✨We're teaching an in-person LLM bootcamp in the SF Bay Area on November 14, 2023. Come join us if you want to see the most up-to-dat...


## 대본

welcome back to full stack deep learning we're now on lab four on experiment management let's dive in in this lab we'll learn why experiment management is so important for ml model development and then how we use experiment management in developing the text recognizer and then finally we'll go in detail through workflows for using a tool called weights and biases to do experiment management for ml model development in order to understand why experiment management is so important for machine learning model development let's kick off an experiment so this cell here trains a new type of model on a new type of data so let's kick this cell off and see what happens while our model is training immediately lots of information starts getting printed to the command line we're seeing information about what's running on our machine information about our model and then metric information starts streaming in about performance on the training set performance on the validation set and all of this information is important and useful for knowing what's going on while our model is training while it's learning from the data but if we don't do any further instrumentation of our training process all this information is going to be lost you can already see that the printed values for metrics like the loss are being overwritten during training and if i restart this notebook the output will disappear or if i'm running this in the command line and i close that window the output will be gone and i might not be able to recreate what the arguments were that i put into my script to train my model we really need to save all of this information to log it and to attach to those logs additional metadata like the state of our git repository or the time stamps at which values were measured so that we can correlate our measurements to each other preserve them over time and maintain a record of what's going on in our experiments in the bad old days you would just do this with google sheets or some other ad hoc solution maybe you'd write it yourself and this is just a lot of time and effort on something that's not really central to the ml model development process it's important for enabling that process but it's not directly connected with what we're doing in model development it's exactly the kind of thing that we want to use frameworks and libraries that already incorporate lots of best practices and solid engineering and allow us to port our code between projects more easily so the classic tool for tracking what's going on during machine learning experiments is tensorboard and in fact we've already been using it when we've been doing our training runs we've been logging all kinds of information so that is accessible via tensorboard we just haven't been looking at it so this section of cells goes through pulling up tensorboard and taking a look at some of the information logged in our experiments tensorboard is kind of like jupiter in that it runs separately from the other things that we're doing it's a separate service that that we launch and then look at from another process we can do it just inside the notebook with a little bit of notebook notebook magic but the same commands also work from a terminal you'll just need to open up a browser and point it at the appropriate host and port let's run these cells and take a look at tensorboard so tensorboard takes all of the metrics that we log over time and can display them as charts so looking through here we can find things like our performance on the training set and the validation set throughout the process of model fitting tensorboard works pretty well for looking at the results of a single experiment but once you start trying to compare across multiple experiments or group across experiments the user experience starts to grid additionally because it's this independent service we need to do a lot of management like for example this cell here that's designed to clean up the tensorboard processes that were launched previously effectively with tensorboard you're running a database to log your information to then a web application on top of it that has a nice graphical user interface and that's a pretty common workflow for web developers but it's outside the skill set of most ml engineers and ends up being kind of a distraction from what you really want to do which is make good machine learning powered products and applications there are lots of different ways to move on from just locally run tensorboard for experiment management this section of the notebook walks through some of the options in detail and talks about their pros and their cons what we at full stack deep learning recommend is a tool called weights and biases in our opinion weights and biases offers the best user experience both in terms of developing logging and adding it to your code base and in the graphical interface for looking at your experiments and offers some of the best integrations with other tools like pi torch lightning keras and even tensorboard and offers the best tools for collaboration using the information about your experiments in a team and one of the biggest benefits is that if you're using one of these frameworks like pytorch lightning or hugging face or keras adding weights and biases to your existing experiment running code is super easy relative to the number of additional features that you get this collection of four or five lines here is the bulk of our weights biases integration into our experiment running script and we'll see what each of these lines of code does for us as we go through a logged experiment in the rest of this lab in order to complete the rest of this notebook you'll need a weight some biases account as with github the free tier is very generous for work that is open to the public and the text recognizer project fits really comfortably within that free tier but it can be a lot more limited for work that is private except for academic accounts again much like github if you're uncomfortable with using if you're uncomfortable with using closed governance and partially closed source tools there's some other recommendations for experiment tracking above including mlflow which is a fully open governance open source project for somebody who's trying to develop a deep learning project across the entire stack the ease of use of weights and biases is really important for maintaining velocity and not getting bogged down on details of how the logging is working so we recommend that you at least create an account and check it out as part of these labs you can run this wanby login cell to get instructions on how to create account or to provide your api key if you already have one in this next cell we're going to launch a similar experiment to the one that we just tried with tensorboard but instead log information to weights and biases this experiment can take between 3 and 10 minutes to run while that's happening make sure to keep reading through the rest of the notebook so we see some new things in our output now there's some information from weights and biases and in particular a link to where we can find the information being logged from our run to weights and biases so let's check that out after we've made it through at least one epoch of training and validation we'll be able to see these charts here so you'll see we've logged some metrics from validation metrics from training and inputs and outputs of our model this information is streaming live so we can see it update in this dashboard as our experiment is running as you can see there's multiple different tabs for viewing the information from a single experiment and let's walk through each of them one at a time we've landed on the charts tab but the high level information about our run is actually in the overview tab so this tab has metadata information like when did the run start how long did it last what operating system and python version were you using and which git repository and what was the state of that git repository if we scroll down to the bottom of the page we can also find our configuration information all of the command line arguments that we passed in which include our model hyper parameters and our trainer parameters and some summary information about what our metrics were like most recently or at the end of the run there's also a tab for system metric information so not metrics about our machine learning model directly like its accuracy or its performance on some data set but metrics about the system on which we are running our training like how much the cpu is being utilized or how much of our memory we are using most importantly for accelerated deep learning there's lots of stats about how well we're using our gpu or gpus so this system only has a single gpu and so we can only see one line here but for systems with multiple gpus we would see utilization temperature memory allocation and other metrics for all the gpus simultaneously this is great for quickly catching performance regressions the next tab the model tab has information about our model the overview and the system metrics we get for free by using weights and biases at all for our logging this model tab we get by adding a single line 1b.watch to our logging the next tab the logs tab captures the output in the terminal so this is great for being able to catch things like error messages or warning messages that might otherwise whiz by as our output gets filled up with things logged during training in fact from this i can see that the training run in the co-op notebook has actually finished because the test metrics have been logged the next tab is the files tab which includes some nice things like a requirements.txt if you're running this locally you'll also see a conda environment yaml file and if you have edited the labs relative to the git repository you'll also see a diff.patch which represents the difference between your git state and what's been committed to the version control system and that's super helpful for being able to catch bugs that were introduced in the middle of developing a model in the middle of creating a commit or a pull request that saved my bacon more than once the logs the files the system metrics and the overview are all things that we get completely for free by including weights and biases in our experiment running script but there's also some things that we get by adding a little bit extra this tab the artifacts tab is one of those things it includes all of the binary files that we're generating during our training which include model checkpoints and the inputs and outputs of the model so the image inputs and the text outputs of our text recognition system these all live among the artifacts on the weights and biases page for our experiment heading back to the notebook where we're running the lab let's take a look at the final set of outputs here we can see those first couple lines that are new log by weights biases then a bunch of information that we've seen logged previously and at the bottom we'll see a quick summary and actually the history of the metrics during the run all in the standard output you'll also see this maybe with a little bit less polish in the output if you run in a terminal we can also see another link back to that page where we can view the results of our experiment in the browser this section of the lab walks through everything that i just showed you in the weights and biases interface all the different tabs what code we added in order to get them and what they're useful for but one thing i wanted to point out is that on top of being able to view this information inside the weights and biases interface we can also view it without having to leave the notebook we can embed almost any page on weights and biases into a notebook using an iframe so let's see what that looks like so here's that same run page that we were just looking at but it's now inside of our colab notebook this is great for being able to share information programmatically play around with which things you're looking at with python and not have to switch context out of your notebook into a web app while you're looking at your results at any time we can also click open page in the top right to take a look at this in a full browser window one more navigation tip if we click around all these are clickable and we'll open the these tabs again inside of the jupyter notebook we can also navigate backwards and forwards using these two arrows at the top in this section of the notebook we take a closer look at those inputs and outputs that have been saved to weights and biases these are not just logged as raw media but in the form of tables that associate the input image with the ground truth label and the model's outputs so we can see we have our three columns with the input image the ground truth label and a predicted label this model wasn't trained for very long so its predictions aren't very good we can adjust the aesthetics aesthetics of the table clicking around making the images larger we can click in on an image and look at it more closely oh this is a pretty hard line to read terrible indiscretions the writing of don juan is what i see it looks like that actually is the ground truth string the predicted string is not exactly that we can also do a little bit of exploratory data analysis in this interface for example i can take this ground truth string and calculate its length or i can create an entirely new column like one that calculates the frequencies with which letters are appearing in the model's outputs looks like the model's outputs contain the letter e and the letter t and spaces quite a bit maybe it's hopping on to just the single letter statistics of our input text getting these tables to work does require a little bit of extra code everything before that that we've seen was using features built into pi torch lightning in the pi torch lightning weights and biases integration you can check out how we implemented this in this section of the notebook that pulls in the relevant classes and methods where experiment management tools really shine relative to something like tensorboard is when you have lots and lots of experiments that are all related to some poor project like training this text recognition system you want to be able to slice and dice them filter them compare them track information over long periods of time and share those results with others so in this section we take a look not just at one experiment that we ran in a few minutes in a collab but at a much longer running project with much longer training runs for hours on multi-gpu systems this project was part of the debugging and feature edition work while updating the fsdl course from 2021 to 2022. so let's pull that page up on the left you can see all the different runs that are part of this project we've actually filtered them down to just the longer training runs using the filter tool and then we kind of customize the charts here to pull out important information like which model had the best performance on the test set in terms of character error rate and also what all of the models character error rates and losses were on the test set if we continue scrolling we can also find metrics over time during training like the training loss in addition we made custom charts where we derive new metrics from the things that we logged like this generalization gap chart that looks at the difference between the training loss and the validation loss when that's strongly negative that means our model is heavily overfitting and we can see that some of our runs have been heavily overfitting that training set we can also look at and compare model inputs and outputs at the end of training on the training set and the validation set here we can see that the final text recognizer is doing pretty well on the training set getting its predicted string to pretty closely match that ground truth string if we keep scrolling down and take a look at the validation data we see that it also transfers pretty well to the validation set at any point while going through this interface we can also click in and take a look at a specific run now we're looking at metrics and charts and all the other information that we walked through previously for this individual run so it's really easy to switch between looking at one run multiple runs to go in and change the filters we're applying to look at different types of runs and so we invite you to play around inside this project take a look at some of the things that we've logged and see some of these weights and biases best practices in action in this section we go into a little bit more detail about how we store and version large binary files including media like those input images and model checkpoints in weights and biases let's pull up the page to look at the artifacts for that quick experiment we ran and let's go ahead and check out one of our model checkpoints the specific version doesn't matter that much so an individual artifact page like this one has all kinds of information about an individual version of a specific collection of files that we're logging to weights and biases so an artifact is kind of like a directory more so than an individual file with a model checkpoint we're really kind of just saving a single file but in general artifacts can store an entire directory of directories and files and then they are linearly versioned so starting from version 0 the first one we logged all the way up to the latest version and with the pytorch lightning integration they also get some useful tags like best to identify the model that had the best performance on the metric that we're tracking and this comes just using a pi torch lightning callback that's generic across experiment management systems the model checkpoint callback when we're looking at an individual version of an artifact we can see all kinds of additional metadata and data about that artifact so for example we can see who created it when they did that which tracked experiment is associated with the creation of this artifact that's all available on this overview tab we can also customize metadata that gets logged with the artifact by default pytorch lightning attaches all of our hyper parameters from the command line and metric values we can also inspect the individual files and folders that are associated with this artifact and we can view the metadata about which runs created which artifacts and which runs used which artifacts by looking in this graph view the lineage view artifact storage is available as part of the free tier of weights and biases the storage limits right now in august of 2022 cover 100 gigabytes of artifacts and experiment data it is possible to delete data and so if you've got good experiment hygiene and you write some scripts you can remove unused artifacts and so make those hundred gigabytes run pretty far there's also a nice interface where you can look at how much you're storing and compare it to your limits on wnb in addition to viewing the information that we've logged in that nice browser interface that we started in or in these embedded iframes inside of our notebooks we can also get programmatic access to the information that we've stored in wnb we'll want to use the wnb api to do that this is a very powerful and flexible api there's a whole section of the lab that walks through how to do a kind of ml ops workflow with this api where you go from the model that is running in production all the way back to the training data that ended up in that model but it could also be used for just simple things like pulling down the data that we just logged in this cell here as a pandas data frame and computing some values for ourself maybe plotting them as well the api also gives us programmatic access to that graph of which runs created which artifacts and which runs used which artifacts created by other runs because we're logging the data that we're seeing during training that's coming out of our pi torch data loaders two weights and biases information that's normally ephemeral like the augmented version of a data point is being tracked and stored and we can access it programmatically and this is super useful for detecting issues in your data processing pipeline in your data augmentation in catching those issues and then resolving them quickly when logging it's really important to include as much information as possible so we had git information we had system information we had specialized things that were logging from inside of our models and it's important to log all that information because if you don't log it when you have it and it disappears you can't recreate it and it's hard to know which particular piece of information is going to be the critical piece that you need to resolve some bug correct an outage or to fundamentally improve the performance of your ml system the downside of this is that all that information that you've logged can sometimes be really overwhelming and it's especially not particularly useful to anybody outside of the project who doesn't know what all these numbers mean what all these different names mean and so in order to make this information a little bit more legible you want to use something that pulls the information out and reformats it kind of like a jupiter notebook incorporates additional media and text around code in order to make that code more comprehensible the feature for this in weights and biases is reports we've heavily used reports in updating the text recognizer from 2021. you can check out some of the reports that we wrote to track the work that we were doing at the link in the notebook we walked through a couple different types of reports and how you might use them in an ml model development workflow so the first one we look at is the dashboard report which just grabs some structured subset of the output from one experiment maybe a couple experiments and it's designed for quickly going and seeing what's going on in an ongoing experiment or quickly comparing two experiments to each other so let's take a look at a dashboard report for one of our training runs so you can see there's a lot less information than there are on those run pages or project pages and there's an attempt to organize it for it to be really quick to see differences to see the difference between a baseline model like these two lines up here and the current model like these two lines down here we can see that the loss is going down faster for the new model than it was in the baseline model these charts are heavily customized here in this dashboard like we've incorporated log scaling so that it's easier to see differences in the loss once the values get low and we can also bring in those system metrics so that we can compare and contrast the sort of system level performance of our code another type of report is kind of like a little bit of extra documentation for a pull request so when you're making a pull request and changing a bunch of model training code or a bunch of data processing code it can be really difficult for somebody reviewing your code to feel confident that the changes you've made are good without seeing some additional media and charts that don't always fit well inside of a pull request and in addition if you just screenshot charts and put them in the pull request then it could be difficult to discover those things they're no longer connected to all the other metrics that you are logging in your experiment management system so we can use these reports in weights and biases to keep all that information together so here's an example of a report i added to a pr to the fsdl code base that checks that while i'm refactoring the code i'm not changing our ability to fit data that the training loss goes down in almost the same way before and after the refactor including links to github like that link in the top right and including links as we scroll down to information log to weights and biases so that the version control system state and the state that we're tracking in our experiment management system are linked to each other and then the last kind of report that we consider is one that incorporates tons of addictional of additional text additional context and information and effort to make really beautiful charts out of the logged information so that we can communicate information not just to other people who are reviewing our code and our pull requests or checking in on the status of our projects but to people outside of our team maybe other stakeholders inside our organization or maybe even to the broader machine learning community so there's tons of really cool examples of these on the weights and biases website the one we look at here is one for the dolly mini project now known as crayon that incorporates tons and tons of really nice juicy details on how dolly mini was trained and what the metrics look like during training what the outputs looked like and this is all in the context of all the information that's logged there and you can click in at any time to take a closer look at what's become one of the most popular open machine learning models in addition to running individual experiments carefully chosen where we want to see the impact of changing some single parameter or changing some single piece of code there are times when we want to run lots and lots of experiments and the most common workflow for that is when we want to optimize the hyper parameters of our machine learning system so these are the things like the learning rate and batch size or the number of layers or really any of these configurable pieces of our system that we don't optimize by gradient descent the best way that people have found to pick what these values should should be you can go with values that are close to what people have found in the past you can kind of carefully build intuition around what those values should be or you can just try lots and lots of them in a big sweep of hyperparameter values and in general at fsdl we recommend you just use the hyperparameter sweeping workflows built into your other tooling so if you're using sagemaker you might have tools for it if you're using ray for scaling up your distributed training there are tools for hyperparameter optimization and ray and there's tools for parameter optimization and weights and biases that are really pretty easy so in order to use the weights and biases hyperparameter optimization tool we just need to write a yaml file so there's an example of one of these yaml files in the lab and it's got lots of comments which makes it look maybe a little bit longer and more complex than it needs to be but effectively we need to specify what command we want to run and what the parameters are and that yaml file basically acts to configure a controller for this hyperparameter suite this controller is a lightweight process that lives on the weights and biases servers and waits to get requests from agents that do the actual work of training our models and seeing how well those hyper parameters work so we can use the exact same code that we used for training our models in this hyperparameter sweep all we need to do is launch an agent on whatever machines we have for participating in this so let's go ahead and launch one here uh so the agent will run just one set of hyper parameters because we passed this argument dash dash count equals one but in general you can just not provide that argument and for many types of hyper parameter sweeps this agent will just run forever trying out different values and you can just terminate it at any time either from the machine or from the weights and biases interface there's lots of neat features of this really really dead simple approach to doing hyper parameter optimization and one of them is that it's really trivially easy to launch two separate agents on one machine with two gpus one agent per gpu or one agent per four gpus however you want to do it just by changing environment variables before calling this wandi agent command so the cuda visible devices environment variable changes which gpus are visible to an individual process and so if you open two terminals and do cuda visible devices equals zero in one and cuda visible devices equals one and the other then your two gpus will run independent experiments for you and you'll be able to run your hyper parameter optimizations twice as fast with very little effort so there's a couple of different exercises associated with this lab um so the first and the simplest one is that you can launch your own agent to contribute to a big hyper parameter sweep that we've started here at full stack deep learning trying out 10 million different potential hyper parameter combinations for this lines of text data and this cnn transformer architecture so if we run this cell it'll pull up a dashboard for that hyper parameter optimization sweep and if we take a look at that dashboard on weights and biases we can see there are some nice charts showing the final value of the validation loss a neat little chart showing which parameters had the most influence on that final value so it looks like the most influential parameter in the first 200 experiments we run is the learning rate though the transformer dimension is also showing up as important bigger transformer dimension means lower validation loss there's also these really nifty parallel coordinates chart uh where you can filter down runs to see which ones have the worst validation losses which ones have the best validation losses and you'll see the charts above are being recalculated so we can see for the runs that had let's say the highest learning rate given that we're only caring about the runs that had the highest learning rate what then becomes the most important parameter for changing the validation loss and what impact do those parameters have so a really nice interface for just some quick exploratory analysis of what comes out of your hyper parameter optimization experiments and then you can pull the data down and do a rigorous statistical dive on it or just say yolo and pick the best performing parameters and start a big training run to contribute to that sweep just change the value of this count it takes about 30 minutes to run an individual run in the sweep to check one configuration of the hyper parameters unless of course it crashes in the first 30 seconds uh due to out of memory errors which happens for some combinations of hyper parameters on some systems the second exercise asks you to get a little bit more familiar with the wand b sdk in python almost everything that we do in the full stack deep learning text recognizer code base uses the integration with pytorch lightning but if you're interested in logging things manually like recreating our image and text logging in those structured tables or if you're interested in using weights biases with a different framework that doesn't have an integration then you'll need to learn some of these methods the third exercise invites you to try and find your own good hyper parameters for the line cnn transformer i'd be especially interested to see if anybody's able to find a set of hyper parameters that are better than the ones that come out of our big hyper parameter optimization sweep if you observe any interesting phenomena while you're training this line cnn transformer grab those charts put them into a wnb report and either share it with other folks in the full stack deep learning community on youtube or on twitter or if you see a bug or some strange behavior open an issue on github include your wnb report or run pages and we'll take a look the last exercise which requires the most additional coding involves using the torch metrics library to add some additional statistics logging around tensors and will give you a chance to see what it looks like to write the code that actually calculates the metrics that get logged to experiment management systems and do a bit of a deeper dive on metrics and logging in pi torch lightning and torch metrics so that's everything for this week's lab on experiment management next time we'll see how to troubleshoot the training of deep neural networks how to do code quality assurance in an ml code base and how to write and run tests for our machine learning system i'm looking forward to it see you then
